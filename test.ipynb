{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test--节点插入、删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value2\n",
      "{'key': 'value'}\n",
      "{'key': 'value2'}\n",
      "{}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#添加节点\n",
    " \n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "G1 = nx.DiGraph()     \n",
    "# G1.add_node(1,key = \"value\")            #建立一个空的无向图G\n",
    "G1.add_nodes_from([(1,{\"key\":\"value\"}),(2, {\"key\":\"value2\"}),3])                  #添加一个节点1\n",
    "G1.add_edges_from([(1,2)])\n",
    "G1.nodes[1]['key']\n",
    "for node_index in G1.successors(1):\n",
    "    print(G1.nodes[node_index]['key'])\n",
    "\n",
    "G2 = nx.DiGraph()\n",
    "G2.add_nodes_from([1,2])\n",
    "G = nx.disjoint_union(G1, G2)\n",
    "\n",
    "for node, node_data in G1.nodes.items():\n",
    "    print(node_data)\n",
    "\n",
    "nx.draw(G, with_labels=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test--后继节点、子图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# # 创建有向图\n",
    "# G = nx.DiGraph()\n",
    "\n",
    "# # 添加边\n",
    "# edges = [\n",
    "#     ('A', 'B'),\n",
    "#     ('A', 'C'),\n",
    "#     ('B', 'D'),\n",
    "#     ('C', 'D'),\n",
    "#     ('C', 'E'),\n",
    "#     ('D', 'E')\n",
    "# ]\n",
    "\n",
    "# G.add_edges_from(edges)\n",
    "\n",
    "# # nx.draw(G, with_labels=True)\n",
    "\n",
    "# # 指定起始节点\n",
    "# start_node = 'A'\n",
    "\n",
    "# # 获取所有直接后继节\n",
    "# successors = list(nx.descendants(G, 'A'))\n",
    "# print(\"Direct successors of '{}': {}\".format(start_node, successors))\n",
    "\n",
    "# # 创建子图的节点列表（包括起始节点和其后继）\n",
    "# subgraph_nodes = [start_node] + successors\n",
    "\n",
    "# # 生成子图\n",
    "# H = G.subgraph(subgraph_nodes)\n",
    "\n",
    "# G.remove_nodes_from([\"E\", \"C\"])\n",
    "\n",
    "\n",
    "# # 打印子图的节点和边\n",
    "# print(\"Subgraph nodes:\", G.nodes())\n",
    "# print(\"Subgraph edges:\", G.edges())\n",
    "\n",
    "# # 可视化子图（可选）\n",
    "# nx.draw(G, with_labels=True)\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "G = nx.complete_graph(5)  # 生成一个完全图\n",
    "nx.draw(G, with_labels=True)  # 绘制图形（带节点标签）\n",
    "plt.savefig(\"graph.png\", format=\"PNG\")  # 保存为PNG文件\n",
    "plt.clf()\n",
    "nx.draw(G, with_labels=True)\n",
    "plt.savefig(\"graph.png\", format=\"PNG\")\n",
    "plt.close()  # 关闭绘图窗口，避免内存占用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test--nextworkx转化为pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Data(x=[3, 1], edge_index=[2, 2])\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def convert_nx_to_pyg(G):\n",
    "    # 获取所有节点并排序，确保顺序一致\n",
    "    nodes = list(G.nodes())\n",
    "    node_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "\n",
    "    # 生成节点特征（例如，每个节点一个特征值为1）\n",
    "    n_nodes = len(nodes)\n",
    "    x = torch.ones(n_nodes, 1, dtype=torch.float32)\n",
    "    print(x)\n",
    "    # 处理边信息\n",
    "    edges = list(G.edges())\n",
    "    sources = [node_idx[e[0]] for e in edges]\n",
    "    targets = [node_idx[e[1]] for e in edges]\n",
    "\n",
    "    edge_index = torch.tensor([sources, targets], dtype=torch.long)\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# 创建示例图\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from([(0, 1), (1, 2)])\n",
    "\n",
    "# 转换图\n",
    "data = convert_nx_to_pyg(G)\n",
    "\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test--自定义环境(继承gym.Env) + DQN + RlLib(强化学习框架)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 09:59:49,469\tINFO worker.py:1841 -- Started a local Ray instance.\n",
      "2025-02-14 09:59:50,425\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-02-14 10:17:25</td></tr>\n",
       "<tr><td>Running for: </td><td>00:17:34.95        </td></tr>\n",
       "<tr><td>Memory:      </td><td>118.0/251.3 GiB    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 4.0/48 CPUs, 0/2 GPUs (0.0/1.0 accelerator_type:A30)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_CustomEnv-v0_5fff5_00000</td><td>RUNNING </td><td>49.52.27.20:34900</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 09:59:50,443\tWARNING algorithm_config.py:4726 -- You are running DQN on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2025-02-14 09:59:50,444\tWARNING algorithm_config.py:4726 -- You are running DQN on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(pid=34900)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "\u001b[36m(DQN pid=34900)\u001b[0m 2025-02-14 09:59:54,019\tWARNING algorithm_config.py:4726 -- You are running DQN on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35038)\u001b[0m 2025-02-14 09:59:57,777\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(DQN pid=34900)\u001b[0m 2025-02-14 09:59:58,257\tWARNING algorithm_config.py:4726 -- You are running DQN on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(DQN pid=34900)\u001b[0m 2025-02-14 09:59:58,273\tWARNING rl_module.py:419 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. All algos use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "\u001b[36m(DQN pid=34900)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[36m(pid=35037)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   logger.deprecation(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   logger.deprecation(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m 2025-02-14 09:59:59,166\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     samples = self._sample(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m               ^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     observations, infos = self._try_env_reset()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 159, in _try_env_reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     raise e\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     obs, infos = self.env.reset()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m                  ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/vector/dict_info_to_list.py\", line 95, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/vector/sync_vector_env.py\", line 183, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 400, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     return super().reset(seed=seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/core.py\", line 328, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     return self.env.reset(seed=seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     result = env.reset(**kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5a53efdf420527e542a1f58e01000000 Worker ID: a7277ab932301c8614c5917b2e11662ffab04fb5d2a2b514048ec028 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 26118 Worker PID: 35037 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:00:59,169\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN pid=34900)\u001b[0m 2025-02-14 09:59:58,242\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   logger.deprecation(\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m 2025-02-14 09:59:59,166\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     samples = self._sample(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     raise e\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:00,452\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:00,452\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:00,453\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:00,453\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:00,453\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:00,453\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:00,453\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=36440)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:02,453\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:04,453\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:04,456\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:04,457\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:04,457\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m 2025-02-14 10:01:02,788\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   logger.deprecation(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   logger.deprecation(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m 2025-02-14 10:01:04,469\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     samples = self._sample(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m               ^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     observations, infos = self._try_env_reset()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 159, in _try_env_reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     raise e\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     obs, infos = self.env.reset()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m                  ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/vector/dict_info_to_list.py\", line 95, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/vector/sync_vector_env.py\", line 183, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 400, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     return super().reset(seed=seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/core.py\", line 328, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     return self.env.reset(seed=seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     result = env.reset(**kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:04,472\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=36438)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   logger.deprecation(\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m 2025-02-14 10:01:04,469\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     samples = self._sample(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     raise e\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5a53efdf420527e542a1f58e01000000 Worker ID: 5ef67aa513d4d4f93ba836981ad094cbdf14dd72d9b9313af2228f34 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 10729 Worker PID: 36439 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:06,461\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:06,461\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:06,461\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:06,461\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:06,462\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:06,462\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:06,462\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37661)\u001b[0m 2025-02-14 10:02:07,937\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:08,462\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:08,465\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:08,465\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:08,466\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:08,480\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=37663)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m 2025-02-14 10:02:08,477\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m 2025-02-14 10:02:08,022\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5a53efdf420527e542a1f58e01000000 Worker ID: d7a22fde68cfd3ed649c02563cf64124d81bc6f551c796ee806ea41d Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 31367 Worker PID: 37662 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:10,470\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:10,470\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:10,470\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:10,470\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:10,470\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:10,470\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:10,470\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:12,470\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:12,473\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:12,474\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:12,474\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffe714c762e5b6504c5ec14c0d01000000 Worker ID: a5afbe94be16f3ce67210d6447a65000ed8a65174b3b56f723cfd0f6 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 33585 Worker PID: 38923 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:12,488\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=38922)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m 2025-02-14 10:03:12,485\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m 2025-02-14 10:03:12,114\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:14,477\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:14,478\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:14,478\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:14,478\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:14,478\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:14,478\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:14,478\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:16,478\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:16,481\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:16,481\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:16,481\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffe714c762e5b6504c5ec14c0d01000000 Worker ID: 7d521623678ef5a150dfab1c3eb58e4d418350ce839d92a30df5ae49 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 28264 Worker PID: 40150 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:16,496\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=40149)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m 2025-02-14 10:04:16,493\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40148)\u001b[0m 2025-02-14 10:04:16,081\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:18,486\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:18,486\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:18,486\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:18,486\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:18,486\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:18,486\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:18,486\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:20,487\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:20,490\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:20,490\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:20,490\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:20,505\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=41244)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m 2025-02-14 10:05:20,502\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m 2025-02-14 10:05:20,211\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffe714c762e5b6504c5ec14c0d01000000 Worker ID: 47354d8261ac8bcaf0cf8b7a0088ecd966cb92abdedfd63dc3bfb535 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 36708 Worker PID: 41242 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:22,495\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:22,496\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: RpcError: RPC Error message: Cancelling all calls; RPC Error details: . The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:22,496\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:22,496\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:22,496\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: RpcError: RPC Error message: Cancelling all calls; RPC Error details: . The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:22,496\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:22,496\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:24,496\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:24,499\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:24,499\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:24,499\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff7c669962a10c460863c7229c01000000 Worker ID: 8443272f76022987cf227401bcdc0d538f0476f7d4a0dc02f4db1eb0 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 11942 Worker PID: 42396 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:24,514\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=42396)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m 2025-02-14 10:06:24,511\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m 2025-02-14 10:06:24,171\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:26,505\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:26,505\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:26,505\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:26,505\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:26,505\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:26,505\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:26,505\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:28,505\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:28,509\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:28,509\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:28,509\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffe714c762e5b6504c5ec14c0d01000000 Worker ID: 249f851cc279d3e943389c3728560611ced62b5addba96b4b2397e2b Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 22493 Worker PID: 43606 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:28,523\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=43605)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m 2025-02-14 10:07:28,520\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43605)\u001b[0m 2025-02-14 10:07:28,128\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:30,514\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:30,514\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:30,514\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:30,515\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:30,515\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:30,515\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:30,515\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:32,515\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:32,518\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:32,518\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:32,518\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffe714c762e5b6504c5ec14c0d01000000 Worker ID: 5218125dc9b298d5c2778043e0dd808f94b139f62a7b13506f527b03 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 29905 Worker PID: 44767 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:32,532\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=44767)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m 2025-02-14 10:08:32,529\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m 2025-02-14 10:08:32,116\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:34,522\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:34,523\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:34,523\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:34,523\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:34,523\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:34,523\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:34,523\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:36,523\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:36,526\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:36,526\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:36,526\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5a53efdf420527e542a1f58e01000000 Worker ID: 4669a199656d83527259bb10707a2138ba99af593ba528754e3f84ba Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 30943 Worker PID: 45887 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:36,540\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=45887)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m 2025-02-14 10:09:36,538\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m 2025-02-14 10:09:36,182\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:38,530\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: RpcError: RPC Error message: Cancelling all calls; RPC Error details: . The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:38,530\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:38,531\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:38,531\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: RpcError: RPC Error message: Cancelling all calls; RPC Error details: . The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:38,531\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:38,531\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:38,531\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:40,531\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:40,534\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:40,534\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:40,534\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5a53efdf420527e542a1f58e01000000 Worker ID: b7118d7f1aae3fbf366677981d6e234e3420f8675b00cce6eb3d2b72 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 14141 Worker PID: 47097 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:40,548\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=47097)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m 2025-02-14 10:10:40,546\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m 2025-02-14 10:10:40,100\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:42,540\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:42,540\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:42,540\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:42,540\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:42,540\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:42,540\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:42,540\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:44,540\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:44,544\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:44,544\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:44,544\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffe714c762e5b6504c5ec14c0d01000000 Worker ID: ac65c1da51714d51090f3f7ce1d4f6ca251ddb9bf4edc4c2de2ce531 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 10445 Worker PID: 48295 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:44,559\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=48293)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m 2025-02-14 10:11:44,556\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m 2025-02-14 10:11:44,121\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:46,549\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:46,549\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:46,550\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:46,550\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:46,550\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:46,550\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:46,550\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:48,549\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:48,552\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:48,552\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:48,553\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5a53efdf420527e542a1f58e01000000 Worker ID: 4f3982e098a1fb96efa285f248d2b147ca185b3c6ce46caa73351e5d Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 22975 Worker PID: 49482 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:48,568\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=49482)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m 2025-02-14 10:12:48,565\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m 2025-02-14 10:12:48,166\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:50,558\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:50,558\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:50,558\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:50,558\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:50,559\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:50,559\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:50,559\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:52,559\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:52,562\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:52,562\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:52,562\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:52,577\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=50641)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m 2025-02-14 10:13:52,574\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m 2025-02-14 10:13:52,257\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff7c669962a10c460863c7229c01000000 Worker ID: e6ee381bebd517f2dd06813eabfeae86f2d3e05de145268f2ac305b2 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 19454 Worker PID: 50639 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:54,568\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:54,568\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:54,568\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:54,569\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:54,569\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:54,569\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:54,569\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:56,569\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:56,572\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:56,572\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:56,572\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5a53efdf420527e542a1f58e01000000 Worker ID: 94d79a6be5cf806aa07e8ae23a53eaf9623463dd0b623bcb6b06c1d9 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 12146 Worker PID: 52032 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:15:56,586\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=52030)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m 2025-02-14 10:14:56,583\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m 2025-02-14 10:14:56,167\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:15:58,578\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:15:58,578\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:15:58,578\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:15:58,578\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:15:58,579\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:15:58,579\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:15:58,579\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:16:00,579\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:16:00,582\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:16:00,582\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:16:00,582\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:00,597\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=53278)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m 2025-02-14 10:16:00,594\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53276)\u001b[0m 2025-02-14 10:16:00,177\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5a53efdf420527e542a1f58e01000000 Worker ID: 17e7458f38eb0ea1075e38a3bcbe7e1eaae40831e229ecfdb446fc58 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 27299 Worker PID: 53278 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:02,588\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:02,588\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: RpcError: RPC Error message: Cancelling all calls; RPC Error details: . The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:02,588\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:02,588\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:02,589\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: RpcError: RPC Error message: Cancelling all calls; RPC Error details: . The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:02,589\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:02,589\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:04,589\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:04,592\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:04,592\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:04,592\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n",
      "2025-02-14 10:17:25,385\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2025-02-14 10:17:25,388\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/data/homedata/lch/ray_results/DQN_2025-02-14_09-59-50' in 0.0019s.\n",
      "2025-02-14 10:17:35,395\tINFO tune.py:1041 -- Total run time: 1064.97 seconds (1054.95 seconds for the tuning loop).\n",
      "2025-02-14 10:17:35,396\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n",
      "\u001b[36m(pid=54468)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m 2025-02-14 10:17:04,603\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m 2025-02-14 10:17:04,176\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff7c669962a10c460863c7229c01000000 Worker ID: 7caa363a18c1897ff15224680c4e41a00982b2fcd0d550e4b8334814 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 32405 Worker PID: 53276 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class CustomEnv(gymnasium.Env):\n",
    "    metadata = {'render.modes': ['console']}\n",
    "\n",
    "    def __init__(self, seed=None):\n",
    "        super().__init__()\n",
    "        self.n_actions = 2  # 动作空间的维度\n",
    "        self.n_states = 3   # 状态空间的维度\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(self.n_states,), dtype=np.float32)\n",
    "\n",
    "        self._seed = seed\n",
    "        if self._seed is not None:\n",
    "            random.seed(self._seed)\n",
    "            np.random.seed(self._seed)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment and return the initial state\n",
    "        self.state = np.array([np.random.uniform(-1, 1) for _ in range(self.n_states)])\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Execute one step in the environment\n",
    "        reward = random.uniform(0, 1)\n",
    "        done = False  # Episodes are not terminal by default\n",
    "        next_state = np.array([np.random.uniform(-1, 1) for _ in range(self.n_states)])\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def render(self, mode='console'):\n",
    "        print(f\"Current state: {self.state}\")\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def is_continuous(self):\n",
    "        return False\n",
    "\n",
    "    def is_discrete(self):\n",
    "        return True\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "### DQN算法实现部分（使用PyTorch）\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DQN_Agent:\n",
    "    def __init__(self, env, batch_size=32, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=10000):\n",
    "        self.env = env\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        input_size = env.observation_space.shape[0]\n",
    "        output_size = env.action_space.n\n",
    "        self.model = DQN(input_size, output_size)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randint(0, self.env.action_space.n - 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(torch.FloatTensor(state))\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([t[0] for t in minibatch])\n",
    "        actions = np.array([t[1] for t in minibatch]).astype(np.int64)\n",
    "        rewards = np.array([t[2] for t in minibatch]).astype(np.float32)\n",
    "        next_states = np.array([t[3] for t in minibatch])\n",
    "        dones = np.array([t[4] for t in minibatch]).astype(bool)\n",
    "\n",
    "        # Convert to tensors\n",
    "        states_t = torch.FloatTensor(states)\n",
    "        actions_t = actions.long()\n",
    "        rewards_t = rewards.unsqueeze(1)  # 增加一个维度，方便后续计算\n",
    "        next_states_t = torch.FloatTensor(next_states)\n",
    "        dones_t = dones.float().unsqueeze(1)\n",
    "\n",
    "        current_q_values = self.model(states_t).gather(dim=1, index=actions_t.unsqueeze(1))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.model(next_states_t).max(dim=1)[0].detach()\n",
    "\n",
    "        target_q_values = rewards_t + (1 - dones_t) * self.gamma * next_q_values.unsqueeze(1)\n",
    "\n",
    "        loss = self.criterion(current_q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon_decay / (self.epsilon_decay + np.exp(-self.epsilon)))\n",
    "\n",
    "### 使用RLlib整合代码\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.env import BaseEnv\n",
    "\n",
    "# 将自定义环境注册到Ray/RLLib\n",
    "def env_creator(env_config):\n",
    "    return CustomEnv()\n",
    "\n",
    "register_env(\"CustomEnv-v0\", env_creator)\n",
    "\n",
    "# DQN模型的配置\n",
    "config = {\n",
    "    \"env\": \"CustomEnv-v0\",\n",
    "    \"num_workers\": 3,\n",
    "    \"gamma\": 0.99,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"buffer_size\": 10000,\n",
    "    \"batch_size\": 32,\n",
    "    \"epsilon_start\": 1.0,\n",
    "    \"epsilon_end\": 0.01,\n",
    "    \"epsilon_decay\": 10000,\n",
    "}\n",
    "\n",
    "ray.init()\n",
    "\n",
    "# 启动训练过程\n",
    "tune.run(\n",
    "    \"DQN\",\n",
    "    config=config,\n",
    "    stop={\"episode_reward_mean\": 500},\n",
    ")\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test--numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 4.]\n",
      " [5. 6.]]\n",
      "(1, 1)\n",
      "[0]\n",
      "2\n",
      "8\n",
      "1.0986122886681096\n",
      "0.11111111111111112\n",
      "0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "v1 = np.zeros((0,2))\n",
    "v2 = np.array([3,4]).reshape(1,-1)\n",
    "v3 = np.array([5, 6]).reshape(1,-1)\n",
    "v = np.vstack((v1, v2,v3))\n",
    "print(v)\n",
    "# x = x.softmax(0).detach().numpy()\n",
    "# np.random.choice(x, 1, p=x)\n",
    "print(np.unravel_index(3, (2,2)))\n",
    "probs = [1/6,2/6,3/6]\n",
    "action_idx = np.random.choice(range(0,len(probs)), 1, p=probs)\n",
    "print(action_idx)\n",
    "action_idx = np.argmax(probs)\n",
    "print(action_idx)\n",
    "print(np.power(2,3))\n",
    "print(np.log(3))\n",
    "print(np.power(2,-2*np.log2(3)))\n",
    "x = np.random.randint(0,2)\n",
    "print(x)\n",
    "x = [1,2,3,4]\n",
    "print(x[(1,2)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test--Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "tensor([1., 2., 2., 3.])\n",
      "torch.float32\n",
      "tensor([0.3374, 0.5713, 0.2187, 0.8549, 0.0842, 0.6109, 0.0438, 0.6927])\n",
      "tensor([0.3374, 0.5713, 0.2187, 0.8549, 0.0842, 0.6109, 0.0438, 0.6927])\n",
      "torch.Size([4, 2])\n",
      "tensor([[0., 2.],\n",
      "        [2., 3.]])\n",
      "torch.Size([2, 1, 3])\n",
      "tensor([], size=(0, 2, 2))\n",
      "tensor([[-0.1331,  1.4879, -1.0386,  1.5351],\n",
      "        [ 0.7848, -0.1026,  0.5346, -0.7481],\n",
      "        [-2.0278,  1.2066,  0.1080, -0.4215]])\n",
      "torch.Size([4, 2, 2])\n",
      "tensor([1, 3])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([6])\n",
      "tensor([[1],\n",
      "        [3]])\n",
      "tensor([3.7000, 5.6000])\n",
      "tensor([ True, False,  True])\n",
      "PackedSequence(data=tensor([[1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [3, 4, 5],\n",
      "        [2, 3, 4]]), batch_sizes=tensor([3, 1]), sorted_indices=None, unsorted_indices=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "torch.cuda.device_count()\n",
    "v1 = torch.Tensor([1,2])\n",
    "v2 = torch.Tensor([3,4])\n",
    "v = torch.concatenate((v1,v2))\n",
    "print(v.shape)\n",
    "v = torch.Tensor([[1,2],[2,3]])\n",
    "print(v.view(4))\n",
    "x = torch.randn(64, 32)\n",
    "print(x.dtype)\n",
    "x = torch.rand((2,4))\n",
    "x = x.view(-1)\n",
    "print(x)\n",
    "# flag = torch.Tensor([1,0])\n",
    "# x = torch.where(flag > 0, x, torch.tensor(float(\"-inf\")))\n",
    "print(x)\n",
    "x = torch.randn((2,2))\n",
    "y = torch.randn((2,2))\n",
    "z = torch.cat([x,y],dim=0)\n",
    "print(z.shape)\n",
    "# features = torch.randn((135, 7))\n",
    "# pad_value = torch.randn((1,1,135))\n",
    "# pad_value = pad_value.expand(135,-1,)\n",
    "# print(pad_value.shape)\n",
    "# padded_features = torch.cat([pad_value.expand(features.shape[0], -1,-1), features], dim=1)\n",
    "x = torch.Tensor([[1,2], [2,3]])\n",
    "x[0][0] = 0\n",
    "print(x)\n",
    "x = torch.randn(1,2,3)\n",
    "x = x.transpose(0,1)\n",
    "print(x.shape)\n",
    "x = torch.empty((0,2,2))\n",
    "print(x)\n",
    "x = torch.randn(3,2,2)\n",
    "print(x.view(3,-1))\n",
    "y = torch.randn((2,2))\n",
    "y = y.unsqueeze(0)\n",
    "y = torch.cat((x,y), dim = 0)\n",
    "print(y.shape)\n",
    "x = torch.tensor([1,2,3])\n",
    "y = [True, False, True]\n",
    "print(x[y])\n",
    "x = torch.randn((2,2))\n",
    "y = torch.randn((2,2))\n",
    "z = torch.concat((x,y), dim = 1)\n",
    "print(z.shape)\n",
    "x = torch.randn((1,3,2))\n",
    "x = x.view(-1)\n",
    "print(x.shape)\n",
    "x = torch.tensor([[1,2],[2,3]])\n",
    "y = x.gather(index = torch.tensor([[0],[1]]),dim = 1)\n",
    "print(y)\n",
    "x = torch.tensor([[1,2,3],[2,1,4]])\n",
    "y = x.max(1)[0]\n",
    "r = torch.tensor([1,2])\n",
    "y = r+y*0.9\n",
    "print(y)\n",
    "t = torch.empty(0, dtype = bool)\n",
    "x = [True, False, True]\n",
    "for each in x:\n",
    "    t = torch.concat((t, torch.tensor([each], dtype = bool)))\n",
    "print(t)\n",
    "sentences = [[[1,2,3],[2,3,4]],[[1,2,3]],[[3,4,5]]]\n",
    "ps = pad_sequence([torch.tensor(s) for s in sentences], batch_first=True, padding_value=0)\n",
    "seq_lengths = [2, 1, 1]\n",
    "padded_sentences = pack_padded_sequence(ps, seq_lengths, batch_first=True)\n",
    "\n",
    "print(padded_sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test--强化学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<第0周期\n",
      "episode:0,reward_sum:31.0\n",
      "<<<<<<<<<第1周期\n",
      "episode:1,reward_sum:28.0\n",
      "<<<<<<<<<第2周期\n",
      "episode:2,reward_sum:29.0\n",
      "<<<<<<<<<第3周期\n",
      "episode:3,reward_sum:27.0\n",
      "<<<<<<<<<第4周期\n",
      "episode:4,reward_sum:29.0\n",
      "<<<<<<<<<第5周期\n",
      "episode:5,reward_sum:31.0\n",
      "<<<<<<<<<第6周期\n",
      "episode:6,reward_sum:29.0\n",
      "<<<<<<<<<第7周期\n",
      "episode:7,reward_sum:35.0\n",
      "<<<<<<<<<第8周期\n",
      "episode:8,reward_sum:26.0\n",
      "<<<<<<<<<第9周期\n",
      "episode:9,reward_sum:24.0\n",
      "<<<<<<<<<第10周期\n",
      "episode:10,reward_sum:25.0\n",
      "<<<<<<<<<第11周期\n",
      "episode:11,reward_sum:39.0\n",
      "<<<<<<<<<第12周期\n",
      "episode:12,reward_sum:26.0\n",
      "<<<<<<<<<第13周期\n",
      "episode:13,reward_sum:21.0\n",
      "<<<<<<<<<第14周期\n",
      "episode:14,reward_sum:29.0\n",
      "<<<<<<<<<第15周期\n",
      "episode:15,reward_sum:26.0\n",
      "<<<<<<<<<第16周期\n",
      "episode:16,reward_sum:26.0\n",
      "<<<<<<<<<第17周期\n",
      "episode:17,reward_sum:27.0\n",
      "<<<<<<<<<第18周期\n",
      "episode:18,reward_sum:11.0\n",
      "<<<<<<<<<第19周期\n",
      "episode:19,reward_sum:12.0\n",
      "<<<<<<<<<第20周期\n",
      "episode:20,reward_sum:24.0\n",
      "<<<<<<<<<第21周期\n",
      "episode:21,reward_sum:22.0\n",
      "<<<<<<<<<第22周期\n",
      "episode:22,reward_sum:10.0\n",
      "<<<<<<<<<第23周期\n",
      "episode:23,reward_sum:18.0\n",
      "<<<<<<<<<第24周期\n",
      "episode:24,reward_sum:9.0\n",
      "<<<<<<<<<第25周期\n",
      "episode:25,reward_sum:78.0\n",
      "<<<<<<<<<第26周期\n",
      "episode:26,reward_sum:87.0\n",
      "<<<<<<<<<第27周期\n",
      "episode:27,reward_sum:60.0\n",
      "<<<<<<<<<第28周期\n",
      "episode:28,reward_sum:76.0\n",
      "<<<<<<<<<第29周期\n",
      "episode:29,reward_sum:246.0\n",
      "<<<<<<<<<第30周期\n",
      "episode:30,reward_sum:106.0\n",
      "<<<<<<<<<第31周期\n",
      "episode:31,reward_sum:129.0\n",
      "<<<<<<<<<第32周期\n",
      "episode:32,reward_sum:266.0\n",
      "<<<<<<<<<第33周期\n",
      "episode:33,reward_sum:417.0\n",
      "<<<<<<<<<第34周期\n",
      "episode:34,reward_sum:417.0\n",
      "<<<<<<<<<第35周期\n",
      "episode:35,reward_sum:385.0\n",
      "<<<<<<<<<第36周期\n",
      "episode:36,reward_sum:682.0\n",
      "<<<<<<<<<第37周期\n",
      "episode:37,reward_sum:713.0\n",
      "<<<<<<<<<第38周期\n",
      "episode:38,reward_sum:1751.0\n",
      "<<<<<<<<<第39周期\n",
      "episode:39,reward_sum:784.0\n",
      "<<<<<<<<<第40周期\n",
      "episode:40,reward_sum:1364.0\n",
      "<<<<<<<<<第41周期\n",
      "episode:41,reward_sum:1380.0\n",
      "<<<<<<<<<第42周期\n",
      "episode:42,reward_sum:679.0\n",
      "<<<<<<<<<第43周期\n",
      "episode:43,reward_sum:1903.0\n",
      "<<<<<<<<<第44周期\n",
      "episode:44,reward_sum:1373.0\n",
      "<<<<<<<<<第45周期\n",
      "episode:45,reward_sum:1111.0\n",
      "<<<<<<<<<第46周期\n",
      "episode:46,reward_sum:494.0\n",
      "<<<<<<<<<第47周期\n",
      "episode:47,reward_sum:1490.0\n",
      "<<<<<<<<<第48周期\n",
      "episode:48,reward_sum:1001.0\n",
      "<<<<<<<<<第49周期\n",
      "episode:49,reward_sum:1229.0\n",
      "<<<<<<<<<第50周期\n",
      "episode:50,reward_sum:2155.0\n",
      "<<<<<<<<<第51周期\n",
      "episode:51,reward_sum:807.0\n",
      "<<<<<<<<<第52周期\n",
      "episode:52,reward_sum:1097.0\n",
      "<<<<<<<<<第53周期\n",
      "episode:53,reward_sum:1499.0\n",
      "<<<<<<<<<第54周期\n",
      "episode:54,reward_sum:570.0\n",
      "<<<<<<<<<第55周期\n",
      "episode:55,reward_sum:3322.0\n",
      "<<<<<<<<<第56周期\n",
      "episode:56,reward_sum:1243.0\n",
      "<<<<<<<<<第57周期\n",
      "episode:57,reward_sum:1467.0\n",
      "<<<<<<<<<第58周期\n",
      "episode:58,reward_sum:1347.0\n",
      "<<<<<<<<<第59周期\n",
      "episode:59,reward_sum:899.0\n",
      "<<<<<<<<<第60周期\n",
      "episode:60,reward_sum:580.0\n",
      "<<<<<<<<<第61周期\n",
      "episode:61,reward_sum:1957.0\n",
      "<<<<<<<<<第62周期\n",
      "episode:62,reward_sum:2411.0\n",
      "<<<<<<<<<第63周期\n",
      "episode:63,reward_sum:1424.0\n",
      "<<<<<<<<<第64周期\n",
      "episode:64,reward_sum:1109.0\n",
      "<<<<<<<<<第65周期\n",
      "episode:65,reward_sum:1296.0\n",
      "<<<<<<<<<第66周期\n",
      "episode:66,reward_sum:629.0\n",
      "<<<<<<<<<第67周期\n",
      "episode:67,reward_sum:1780.0\n",
      "<<<<<<<<<第68周期\n",
      "episode:68,reward_sum:896.0\n",
      "<<<<<<<<<第69周期\n",
      "episode:69,reward_sum:1150.0\n",
      "<<<<<<<<<第70周期\n",
      "episode:70,reward_sum:1436.0\n",
      "<<<<<<<<<第71周期\n",
      "episode:71,reward_sum:2706.0\n",
      "<<<<<<<<<第72周期\n",
      "episode:72,reward_sum:767.0\n",
      "<<<<<<<<<第73周期\n",
      "episode:73,reward_sum:788.0\n",
      "<<<<<<<<<第74周期\n",
      "episode:74,reward_sum:609.0\n",
      "<<<<<<<<<第75周期\n",
      "episode:75,reward_sum:1615.0\n",
      "<<<<<<<<<第76周期\n",
      "episode:76,reward_sum:1964.0\n",
      "<<<<<<<<<第77周期\n",
      "episode:77,reward_sum:922.0\n",
      "<<<<<<<<<第78周期\n",
      "episode:78,reward_sum:2118.0\n",
      "<<<<<<<<<第79周期\n",
      "episode:79,reward_sum:3641.0\n",
      "<<<<<<<<<第80周期\n",
      "episode:80,reward_sum:993.0\n",
      "<<<<<<<<<第81周期\n",
      "episode:81,reward_sum:3746.0\n",
      "<<<<<<<<<第82周期\n",
      "episode:82,reward_sum:926.0\n",
      "<<<<<<<<<第83周期\n",
      "episode:83,reward_sum:1185.0\n",
      "<<<<<<<<<第84周期\n",
      "episode:84,reward_sum:3834.0\n",
      "<<<<<<<<<第85周期\n",
      "episode:85,reward_sum:2138.0\n",
      "<<<<<<<<<第86周期\n",
      "episode:86,reward_sum:1657.0\n",
      "<<<<<<<<<第87周期\n",
      "episode:87,reward_sum:1030.0\n",
      "<<<<<<<<<第88周期\n",
      "episode:88,reward_sum:1118.0\n",
      "<<<<<<<<<第89周期\n",
      "episode:89,reward_sum:1258.0\n",
      "<<<<<<<<<第90周期\n",
      "episode:90,reward_sum:1250.0\n",
      "<<<<<<<<<第91周期\n",
      "episode:91,reward_sum:1181.0\n",
      "<<<<<<<<<第92周期\n",
      "episode:92,reward_sum:1274.0\n",
      "<<<<<<<<<第93周期\n",
      "episode:93,reward_sum:945.0\n",
      "<<<<<<<<<第94周期\n",
      "episode:94,reward_sum:2662.0\n",
      "<<<<<<<<<第95周期\n",
      "episode:95,reward_sum:2143.0\n",
      "<<<<<<<<<第96周期\n",
      "episode:96,reward_sum:1449.0\n",
      "<<<<<<<<<第97周期\n",
      "episode:97,reward_sum:719.0\n",
      "<<<<<<<<<第98周期\n",
      "episode:98,reward_sum:1254.0\n",
      "<<<<<<<<<第99周期\n",
      "episode:99,reward_sum:806.0\n",
      "<<<<<<<<<第100周期\n",
      "episode:100,reward_sum:598.0\n",
      "<<<<<<<<<第101周期\n",
      "episode:101,reward_sum:1398.0\n",
      "<<<<<<<<<第102周期\n",
      "episode:102,reward_sum:1440.0\n",
      "<<<<<<<<<第103周期\n",
      "episode:103,reward_sum:3697.0\n",
      "<<<<<<<<<第104周期\n",
      "episode:104,reward_sum:600.0\n",
      "<<<<<<<<<第105周期\n",
      "episode:105,reward_sum:1813.0\n",
      "<<<<<<<<<第106周期\n",
      "episode:106,reward_sum:1389.0\n",
      "<<<<<<<<<第107周期\n",
      "episode:107,reward_sum:1350.0\n",
      "<<<<<<<<<第108周期\n",
      "episode:108,reward_sum:1625.0\n",
      "<<<<<<<<<第109周期\n",
      "episode:109,reward_sum:1061.0\n",
      "<<<<<<<<<第110周期\n",
      "episode:110,reward_sum:958.0\n",
      "<<<<<<<<<第111周期\n",
      "episode:111,reward_sum:3124.0\n",
      "<<<<<<<<<第112周期\n",
      "episode:112,reward_sum:808.0\n",
      "<<<<<<<<<第113周期\n",
      "episode:113,reward_sum:613.0\n",
      "<<<<<<<<<第114周期\n",
      "episode:114,reward_sum:1979.0\n",
      "<<<<<<<<<第115周期\n",
      "episode:115,reward_sum:621.0\n",
      "<<<<<<<<<第116周期\n",
      "episode:116,reward_sum:889.0\n",
      "<<<<<<<<<第117周期\n",
      "episode:117,reward_sum:5781.0\n",
      "<<<<<<<<<第118周期\n",
      "episode:118,reward_sum:1319.0\n",
      "<<<<<<<<<第119周期\n",
      "episode:119,reward_sum:732.0\n",
      "<<<<<<<<<第120周期\n",
      "episode:120,reward_sum:1029.0\n",
      "<<<<<<<<<第121周期\n",
      "episode:121,reward_sum:916.0\n",
      "<<<<<<<<<第122周期\n",
      "episode:122,reward_sum:1514.0\n",
      "<<<<<<<<<第123周期\n",
      "episode:123,reward_sum:1122.0\n",
      "<<<<<<<<<第124周期\n",
      "episode:124,reward_sum:1514.0\n",
      "<<<<<<<<<第125周期\n",
      "episode:125,reward_sum:795.0\n",
      "<<<<<<<<<第126周期\n",
      "episode:126,reward_sum:1873.0\n",
      "<<<<<<<<<第127周期\n",
      "episode:127,reward_sum:712.0\n",
      "<<<<<<<<<第128周期\n",
      "episode:128,reward_sum:1475.0\n",
      "<<<<<<<<<第129周期\n",
      "episode:129,reward_sum:916.0\n",
      "<<<<<<<<<第130周期\n",
      "episode:130,reward_sum:1024.0\n",
      "<<<<<<<<<第131周期\n",
      "episode:131,reward_sum:960.0\n",
      "<<<<<<<<<第132周期\n",
      "episode:132,reward_sum:861.0\n",
      "<<<<<<<<<第133周期\n",
      "episode:133,reward_sum:814.0\n",
      "<<<<<<<<<第134周期\n",
      "episode:134,reward_sum:1419.0\n",
      "<<<<<<<<<第135周期\n",
      "episode:135,reward_sum:1061.0\n",
      "<<<<<<<<<第136周期\n",
      "episode:136,reward_sum:984.0\n",
      "<<<<<<<<<第137周期\n",
      "episode:137,reward_sum:962.0\n",
      "<<<<<<<<<第138周期\n",
      "episode:138,reward_sum:892.0\n",
      "<<<<<<<<<第139周期\n",
      "episode:139,reward_sum:1326.0\n",
      "<<<<<<<<<第140周期\n",
      "episode:140,reward_sum:867.0\n",
      "<<<<<<<<<第141周期\n",
      "episode:141,reward_sum:2923.0\n",
      "<<<<<<<<<第142周期\n",
      "episode:142,reward_sum:1294.0\n",
      "<<<<<<<<<第143周期\n",
      "episode:143,reward_sum:1584.0\n",
      "<<<<<<<<<第144周期\n",
      "episode:144,reward_sum:705.0\n",
      "<<<<<<<<<第145周期\n",
      "episode:145,reward_sum:1757.0\n",
      "<<<<<<<<<第146周期\n",
      "episode:146,reward_sum:918.0\n",
      "<<<<<<<<<第147周期\n",
      "episode:147,reward_sum:2503.0\n",
      "<<<<<<<<<第148周期\n",
      "episode:148,reward_sum:887.0\n",
      "<<<<<<<<<第149周期\n",
      "episode:149,reward_sum:1586.0\n",
      "<<<<<<<<<第150周期\n",
      "episode:150,reward_sum:1001.0\n",
      "<<<<<<<<<第151周期\n",
      "episode:151,reward_sum:1223.0\n",
      "<<<<<<<<<第152周期\n",
      "episode:152,reward_sum:2009.0\n",
      "<<<<<<<<<第153周期\n",
      "episode:153,reward_sum:4172.0\n",
      "<<<<<<<<<第154周期\n",
      "episode:154,reward_sum:997.0\n",
      "<<<<<<<<<第155周期\n",
      "episode:155,reward_sum:729.0\n",
      "<<<<<<<<<第156周期\n",
      "episode:156,reward_sum:3782.0\n",
      "<<<<<<<<<第157周期\n",
      "episode:157,reward_sum:2326.0\n",
      "<<<<<<<<<第158周期\n",
      "episode:158,reward_sum:1344.0\n",
      "<<<<<<<<<第159周期\n",
      "episode:159,reward_sum:861.0\n",
      "<<<<<<<<<第160周期\n",
      "episode:160,reward_sum:3402.0\n",
      "<<<<<<<<<第161周期\n",
      "episode:161,reward_sum:1281.0\n",
      "<<<<<<<<<第162周期\n",
      "episode:162,reward_sum:1583.0\n",
      "<<<<<<<<<第163周期\n",
      "episode:163,reward_sum:1158.0\n",
      "<<<<<<<<<第164周期\n",
      "episode:164,reward_sum:2247.0\n",
      "<<<<<<<<<第165周期\n",
      "episode:165,reward_sum:948.0\n",
      "<<<<<<<<<第166周期\n",
      "episode:166,reward_sum:3204.0\n",
      "<<<<<<<<<第167周期\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 112\u001b[0m\n\u001b[1;32m    109\u001b[0m episode_reward_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m              \u001b[38;5;66;03m# 初始化每个周期的reward值\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m                    \u001b[38;5;66;03m# 开启画面\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     a \u001b[38;5;241m=\u001b[39m dqn\u001b[38;5;241m.\u001b[39mchoose_action(s)        \u001b[38;5;66;03m# 与环境互动选择action\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     s_,r,done, info,_\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a)\n",
      "File \u001b[0;32m~/.conda/envs/transformer/lib/python3.9/site-packages/gym-0.26.2-py3.9.egg/gym/envs/classic_control/cartpole.py:298\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    297\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# GPU设置\n",
    "device = 'cpu'\n",
    "# if torch.cuda.is_available():\n",
    "#     device = \"cuda:0\"\n",
    "# else:\n",
    "#     device = \"cpu\"\n",
    "\n",
    "# 超参数\n",
    "BATCH_SIZE = 60                                 # 样本数量\n",
    "LR = 0.01                                       # 学习率\n",
    "EPSILON = 0.9                                   # greedy policy\n",
    "GAMMA = 0.9                                     # reward discount\n",
    "TARGET_REPLACE_ITER = 100                       # 目标网络更新频率(固定不懂的Q网络)\n",
    "MEMORY_CAPACITY = 500                          # 记忆库容量\n",
    "# 和环境相关的参数\n",
    "env = gym.make(\"CartPole-v1\",render_mode=\"human\").unwrapped         # 使用gym库中的环境：CartPole，且打开封装(若想了解该环境，请自行百度)\n",
    "N_state = env.observation_space.shape[0]      # 特征数\n",
    "N_action = env.action_space.n\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(N_state,50)\n",
    "        self.fc1.weight.data.normal_(0,0.1)\n",
    "        self.out = nn.Linear(50,N_action)\n",
    "        self.out.weight.data.normal_(0,0.1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        action_value = self.out(x)\n",
    "        return action_value\n",
    "\n",
    "\n",
    "# 定义DQN类(定义Q网络以及一个固定的Q网络)\n",
    "class DQN(object):\n",
    "    def __init__(self):\n",
    "        # 创建评估网络和目标网络\n",
    "        self.eval_net,self.target_net = Net().to(device),Net().to(device)\n",
    "        self.learn_step_counter = 0  # 学习步数记录\n",
    "        self.memory_counter = 0      # 记忆量计数\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY,N_state*2+2)) # 存储空间初始化，每一组的数据为(s_t,a_t,r_t,s_{t+1})\n",
    "        self.optimazer = torch.optim.Adam(self.eval_net.parameters(),lr=LR)\n",
    "        self.loss_func = nn.MSELoss()     # 使用均方损失函数 (loss(xi, yi)=(xi-yi)^2)\n",
    "        self.loss_func = self.loss_func.to(device)\n",
    "\n",
    "    def choose_action(self,x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x),0).to(device)  # 将x转换成32-bit floating point形式，并在dim=0增加维数为1的维度\n",
    "        # 设置探索机制\n",
    "        if np.random.uniform()< EPSILON:\n",
    "            # 若小于设定值，则采用Q中的最优方法\n",
    "            action_value = self.eval_net(x)\n",
    "            # 选定action\n",
    "            action = torch.max(action_value,1)[1].data.cpu().numpy() # 输出每一行最大值的索引，并转化为numpy ndarray形式\n",
    "            action = action[0]\n",
    "        else:\n",
    "            action = np.random.randint(0,N_action)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def store_transition(self,s,a,r,s_):\n",
    "        transition = np.hstack((s,[a,r],s_))   # 因为action和reward就只是个值不是列表，所以要在外面套个[]\n",
    "        # 如果记忆满了需要覆盖旧的数据\n",
    "        index = self.memory_counter % MEMORY_CAPACITY   # 确定在buffer中的行数\n",
    "        self.memory[index,:]=transition        # 用新的数据覆盖之前的之前\n",
    "        self.memory_counter +=1\n",
    "\n",
    "    def learn(self):\n",
    "        # 目标网络更新，就是我们固定不动的网络\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())   # 将评价网络的权重参数赋给目标网络\n",
    "        self.learn_step_counter +=1                 # 目标函数的学习次数+1\n",
    "\n",
    "        # 抽buffer中的数据学习\n",
    "        sample_idex = np.random.choice(MEMORY_CAPACITY,BATCH_SIZE)   # 在[0, 2000)内随机抽取32个数，可能会重复,若更改超参数会变更\n",
    "        b_memory = self.memory[sample_idex,:]    # 抽取选中的行数的数据\n",
    "\n",
    "        # 抽取出32个s数据，保存入b_s中\n",
    "        b_s = torch.FloatTensor(b_memory[:,:N_state]).to(device)\n",
    "        # 抽取出32个a数据，保存入b_a中\n",
    "        b_a = torch.LongTensor(b_memory[:,N_state:N_state+1]).to(device)\n",
    "        # 抽取出32个r数据，保存入b_r中\n",
    "        b_r = torch.FloatTensor(b_memory[:,N_state+1:N_state+2]).to(device)\n",
    "        # 抽取出32个s_数据，保存入b_s_中\n",
    "        b_s_ = torch.FloatTensor(b_memory[:,-N_state:]).to(device)\n",
    "\n",
    "        # 获得32个trasition的评估值和目标值，并利用损失函数和优化器进行评估网络参数更新\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)         # 因为已经确定在s时候所走的action，因此选定该action对应的Q值\n",
    "        # q_next 不进行反向传播，故用detach；q_next表示通过目标网络输出32行每个b_s_对应的一系列动作值\n",
    "        q_next = self.target_net(b_s_).detach()\n",
    "        # 先算出目标值q_target，max(1)[0]相当于计算出每一行中的最大值（注意不是上面的索引了,而是一个一维张量了），view()函数让其变成(32,1)\n",
    "        q_target = b_r + GAMMA*q_next.max(1)[0].view(BATCH_SIZE,1)\n",
    "        # 计算损失值\n",
    "        loss = self.loss_func(q_eval,q_target)\n",
    "        self.optimazer.zero_grad()# 清空上一步的残余更新参数值\n",
    "        loss.backward() # 误差方向传播\n",
    "        self.optimazer.step() # 逐步的梯度优化\n",
    "\n",
    "dqn= DQN()\n",
    "\n",
    "for i in range(400):                    # 设置400个episode\n",
    "    print(f\"<<<<<<<<<第{i}周期\")\n",
    "    s,_ = env.reset()                    # 重置环境\n",
    "    episode_reward_sum = 0              # 初始化每个周期的reward值\n",
    "\n",
    "    while True:\n",
    "        env.render()                    # 开启画面\n",
    "        a = dqn.choose_action(s)        # 与环境互动选择action\n",
    "        s_,r,done, info,_= env.step(a)\n",
    "\n",
    "        # 可以修改reward值让其训练速度加快\n",
    "        x, x_dot, theta, theta_dot = s_\n",
    "        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
    "        new_r = r1 + r2\n",
    "\n",
    "        ########\n",
    "        dqn.store_transition(s,a,new_r,s_)  # 储存样本\n",
    "        episode_reward_sum += r\n",
    "\n",
    "        s = s_                          # 进入下一个状态\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:   # 只有在buffer中存满了数据才会学习\n",
    "            dqn.learn()\n",
    "\n",
    "        if done:\n",
    "            print(f\"episode:{i},reward_sum:{episode_reward_sum}\")\n",
    "\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test--训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集: Cora\n",
      "节点数: 2708, 边数: 10556\n",
      "特征维度: 1433, 类别数: 7\n",
      "训练/验证/测试样本数: 140/500/1000\n",
      "Epoch 010, Loss: 1.8625, Train Acc: 0.8786, Val Acc: 0.6300\n",
      "Epoch 020, Loss: 1.7093, Train Acc: 0.9571, Val Acc: 0.7200\n",
      "Epoch 030, Loss: 1.5188, Train Acc: 0.9714, Val Acc: 0.7660\n",
      "Epoch 040, Loss: 1.2693, Train Acc: 0.9643, Val Acc: 0.7720\n",
      "Epoch 050, Loss: 1.0678, Train Acc: 0.9714, Val Acc: 0.7620\n",
      "Epoch 060, Loss: 0.8351, Train Acc: 0.9714, Val Acc: 0.7760\n",
      "Epoch 070, Loss: 0.7545, Train Acc: 0.9786, Val Acc: 0.7700\n",
      "Epoch 080, Loss: 0.6090, Train Acc: 0.9857, Val Acc: 0.7660\n",
      "Epoch 090, Loss: 0.6101, Train Acc: 0.9929, Val Acc: 0.7800\n",
      "Epoch 100, Loss: 0.5198, Train Acc: 0.9857, Val Acc: 0.7880\n",
      "Epoch 110, Loss: 0.4624, Train Acc: 0.9857, Val Acc: 0.7880\n",
      "Epoch 120, Loss: 0.4192, Train Acc: 0.9929, Val Acc: 0.7800\n",
      "Epoch 130, Loss: 0.4088, Train Acc: 0.9929, Val Acc: 0.7840\n",
      "Epoch 140, Loss: 0.3936, Train Acc: 0.9929, Val Acc: 0.7840\n",
      "Epoch 150, Loss: 0.3697, Train Acc: 0.9929, Val Acc: 0.7880\n",
      "Epoch 160, Loss: 0.3432, Train Acc: 1.0000, Val Acc: 0.7960\n",
      "Epoch 170, Loss: 0.3736, Train Acc: 1.0000, Val Acc: 0.7880\n",
      "Epoch 180, Loss: 0.3217, Train Acc: 0.9929, Val Acc: 0.7860\n",
      "Epoch 190, Loss: 0.3305, Train Acc: 1.0000, Val Acc: 0.7860\n",
      "Epoch 200, Loss: 0.2909, Train Acc: 1.0000, Val Acc: 0.7820\n",
      "\n",
      "测试集准确率: 0.8210\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "# -------------------- 1. 数据加载与预处理 --------------------\n",
    "# 加载Cora数据集并应用特征归一化\n",
    "dataset = Planetoid(root='./data/Cora', name='Cora', transform=NormalizeFeatures())\n",
    "data = dataset[0]\n",
    "\n",
    "# 打印数据基本信息\n",
    "print(f\"数据集: {dataset.name}\")\n",
    "print(f\"节点数: {data.num_nodes}, 边数: {data.num_edges}\")\n",
    "print(f\"特征维度: {data.num_node_features}, 类别数: {dataset.num_classes}\")\n",
    "print(f\"训练/验证/测试样本数: {data.train_mask.sum().item()}/{data.val_mask.sum().item()}/{data.test_mask.sum().item()}\")\n",
    "\n",
    "# 检查CUDA可用性\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# data = data.to(device)\n",
    "\n",
    "# -------------------- 2. 模型定义 --------------------\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# 初始化模型与优化器\n",
    "model = GCN(\n",
    "    in_dim=dataset.num_node_features,\n",
    "    hidden_dim=16,\n",
    "    out_dim=dataset.num_classes\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# -------------------- 3. 训练与评估函数 --------------------\n",
    "def train(model, data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def evaluate(model, data, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = (pred[mask] == data.y[mask]).sum().item()\n",
    "        acc = correct / mask.sum().item()\n",
    "    return acc\n",
    "\n",
    "# -------------------- 4. 训练循环 --------------------\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(1, 201):\n",
    "    loss = train(model, data)\n",
    "    train_acc = evaluate(model, data, data.train_mask)\n",
    "    val_acc = evaluate(model, data, data.val_mask)\n",
    "    \n",
    "    # 保存最佳模型\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_gcn_cora.pth')\n",
    "    \n",
    "    # 每10轮打印日志\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:03d}, Loss: {loss:.4f}, \"\n",
    "              f\"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "# -------------------- 5. 最终测试 --------------------\n",
    "model.load_state_dict(torch.load('best_gcn_cora.pth'))\n",
    "test_acc = evaluate(model, data, data.test_mask)\n",
    "print(f\"\\n测试集准确率: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test--其他"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'title']\n",
      "{('alias', 'col')}\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "12\n",
      "[]\n",
      "1\n",
      "3\n",
      "['1']\n",
      "1\n",
      "2\n",
      "3\n",
      "True\n",
      "False\n",
      "{1, 2, 3}\n",
      "1\n",
      "{frozenset({1, 2, 3})}\n",
      "[4]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d = {'key':'value'}\n",
    "list(d.values())[0]\n",
    "col = 't.title'\n",
    "ans = col.split('.')\n",
    "print(ans)\n",
    "ans = set()\n",
    "ans.add(('alias', 'col'))\n",
    "ans.add(('alias', 'col'))\n",
    "print(ans)\n",
    "a = [(1,2),(3,4)]\n",
    "for b,c in a:\n",
    "    print(b)\n",
    "    print(c)\n",
    "a = '123'\n",
    "a = a[:-1]\n",
    "print(a)\n",
    "a = [1,2,3]\n",
    "b = []\n",
    "\n",
    "print(b)\n",
    "a,b = '1','2'\n",
    "print(a)\n",
    "\n",
    "a = {'1':1, '2' : 2}\n",
    "\n",
    "print(sum(list(a.values())))\n",
    "\n",
    "a = []\n",
    "b = 1\n",
    "a.append(f'{b}')\n",
    "print(a)\n",
    "a = set([1,2,3])\n",
    "b = set([2,1,3])\n",
    "for x in a:\n",
    "    print(x)\n",
    "print(a == b)\n",
    "e = []\n",
    "a = {\"key\":\"value\",\"key2\":1}\n",
    "b = {\"key\":\"value\", \"key2\":2}\n",
    "\n",
    "\n",
    "print(a == b)\n",
    "a = set([1,2])\n",
    "b = set([2,3])\n",
    "a = a.union(b)\n",
    "print(a)\n",
    "a = {1:2}\n",
    "print(len(a))\n",
    "a = [[(1,2,3),'haha'],[(5,6,7),'nihao']]\n",
    "\n",
    "s = set()\n",
    "\n",
    "\n",
    "s.add(frozenset({1,2,3}))\n",
    "s.add(frozenset({2,1,3}))\n",
    "print(s)\n",
    "a = [1,2,3,4]\n",
    "print(a[-1:])\n",
    "x = [1,2,3]\n",
    "\n",
    "a = set()\n",
    "a.add(2)\n",
    "a.add(1)\n",
    "print(list(a)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<Episode: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/homedata/lch/.conda/envs/transformer/lib/python3.9/site-packages/gym-0.26.2-py3.9.egg/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/data/homedata/lch/.conda/envs/transformer/lib/python3.9/site-packages/gym-0.26.2-py3.9.egg/gym/envs/classic_control/cartpole.py:211: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v0\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/tmp/ipykernel_29842/443345609.py:57: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  x = torch.unsqueeze(torch.FloatTensor(x), 0)                            # 将x转换成32-bit floating point形式，并在dim=0增加维数为1的维度\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 4 at dim 1 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 114\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:                                                         \u001b[38;5;66;03m# 开始一个episode (每一个循环代表一步)\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()                                                    \u001b[38;5;66;03m# 显示实验动画\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m                                        \u001b[38;5;66;03m# 输入该步对应的状态s，选择动作\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     s_, r, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a)                                 \u001b[38;5;66;03m# 执行动作，获得反馈\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# 修改奖励 (不修改也可以，修改奖励只是为了更快地得到训练好的摆杆)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 57\u001b[0m, in \u001b[0;36mDQN.choose_action\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):                                                 \u001b[38;5;66;03m# 定义动作选择函数 (x为状态)\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m0\u001b[39m)                            \u001b[38;5;66;03m# 将x转换成32-bit floating point形式，并在dim=0增加维数为1的维度\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform() \u001b[38;5;241m<\u001b[39m EPSILON:                                       \u001b[38;5;66;03m# 生成一个在[0, 1)内的随机数，如果小于EPSILON，选择最优动作\u001b[39;00m\n\u001b[1;32m     59\u001b[0m         actions_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_net\u001b[38;5;241m.\u001b[39mforward(x)                            \u001b[38;5;66;03m# 通过对评估网络输入状态x，前向传播获得动作值\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 4 at dim 1 (got 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch                                    # 导入torch\n",
    "import torch.nn as nn                           # 导入torch.nn\n",
    "import torch.nn.functional as F                 # 导入torch.nn.functional\n",
    "import numpy as np                              # 导入numpy\n",
    "import gym                                      # 导入gym\n",
    "\n",
    "# 超参数\n",
    "BATCH_SIZE = 32                                 # 样本数量\n",
    "LR = 0.01                                       # 学习率\n",
    "EPSILON = 0.9                                   # greedy policy\n",
    "GAMMA = 0.9                                     # reward discount\n",
    "TARGET_REPLACE_ITER = 100                       # 目标网络更新频率\n",
    "MEMORY_CAPACITY = 2000                          # 记忆库容量\n",
    "env = gym.make('CartPole-v0',render_mode=\"human\").unwrapped         # 使用gym库中的环境：CartPole，且打开封装(若想了解该环境，请自行百度)\n",
    "N_ACTIONS = env.action_space.n                  # 杆子动作个数 (2个)\n",
    "N_STATES = env.observation_space.shape[0]       # 杆子状态个数 (4个)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "torch.nn是专门为神经网络设计的模块化接口。nn构建于Autograd之上，可以用来定义和运行神经网络。\n",
    "nn.Module是nn中十分重要的类，包含网络各层的定义及forward方法。\n",
    "定义网络：\n",
    "    需要继承nn.Module类，并实现forward方法。\n",
    "    一般把网络中具有可学习参数的层放在构造函数__init__()中。\n",
    "    只要在nn.Module的子类中定义了forward函数，backward函数就会被自动实现(利用Autograd)。\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 定义Net类 (定义网络)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):                                                         # 定义Net的一系列属性\n",
    "        # nn.Module的子类函数必须在构造函数中执行父类的构造函数\n",
    "        super(Net, self).__init__()                                             # 等价与nn.Module.__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, 50)                                      # 设置第一个全连接层(输入层到隐藏层): 状态数个神经元到50个神经元\n",
    "        self.fc1.weight.data.normal_(0, 0.1)                                    # 权重初始化 (均值为0，方差为0.1的正态分布)\n",
    "        self.out = nn.Linear(50, N_ACTIONS)                                     # 设置第二个全连接层(隐藏层到输出层): 50个神经元到动作数个神经元\n",
    "        self.out.weight.data.normal_(0, 0.1)                                    # 权重初始化 (均值为0，方差为0.1的正态分布)\n",
    "\n",
    "    def forward(self, x):                                                       # 定义forward函数 (x为状态)\n",
    "        x = F.relu(self.fc1(x))                                                 # 连接输入层到隐藏层，且使用激励函数ReLU来处理经过隐藏层后的值\n",
    "        actions_value = self.out(x)                                             # 连接隐藏层到输出层，获得最终的输出值 (即动作值)\n",
    "        return actions_value                                                    # 返回动作值\n",
    "\n",
    "\n",
    "# 定义DQN类 (定义两个网络)\n",
    "class DQN(object):\n",
    "    def __init__(self):                                                         # 定义DQN的一系列属性\n",
    "        self.eval_net, self.target_net = Net(), Net()                           # 利用Net创建两个神经网络: 评估网络和目标网络\n",
    "        self.learn_step_counter = 0                                             # for target updating\n",
    "        self.memory_counter = 0                                                 # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))             # 初始化记忆库，一行代表一个transition\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)    # 使用Adam优化器 (输入为评估网络的参数和学习率)\n",
    "        self.loss_func = nn.MSELoss()                                           # 使用均方损失函数 (loss(xi, yi)=(xi-yi)^2)\n",
    "\n",
    "    def choose_action(self, x):                                                 # 定义动作选择函数 (x为状态)\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)                            # 将x转换成32-bit floating point形式，并在dim=0增加维数为1的维度\n",
    "        if np.random.uniform() < EPSILON:                                       # 生成一个在[0, 1)内的随机数，如果小于EPSILON，选择最优动作\n",
    "            actions_value = self.eval_net.forward(x)                            # 通过对评估网络输入状态x，前向传播获得动作值\n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()                # 输出每一行最大值的索引，并转化为numpy ndarray形式\n",
    "            action = action[0]                                                  # 输出action的第一个数\n",
    "        else:                                                                   # 随机选择动作\n",
    "            action = np.random.randint(0, N_ACTIONS)                            # 这里action随机等于0或1 (N_ACTIONS = 2)\n",
    "        return action                                                           # 返回选择的动作 (0或1)\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):                                    # 定义记忆存储函数 (这里输入为一个transition)\n",
    "        transition = np.hstack((s, [a, r], s_))                                 # 在水平方向上拼接数组\n",
    "        # 如果记忆库满了，便覆盖旧的数据\n",
    "        index = self.memory_counter % MEMORY_CAPACITY                           # 获取transition要置入的行数\n",
    "        self.memory[index, :] = transition                                      # 置入transition\n",
    "        self.memory_counter += 1                                                # memory_counter自加1\n",
    "\n",
    "    def learn(self):                                                            # 定义学习函数(记忆库已满后便开始学习)\n",
    "        # 目标网络参数更新\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:                  # 一开始触发，然后每100步触发\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())         # 将评估网络的参数赋给目标网络\n",
    "        self.learn_step_counter += 1                                            # 学习步数自加1\n",
    "\n",
    "        # 抽取记忆库中的批数据\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)            # 在[0, 2000)内随机抽取32个数，可能会重复\n",
    "        b_memory = self.memory[sample_index, :]                                 # 抽取32个索引对应的32个transition，存入b_memory\n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        # 将32个s抽出，转为32-bit floating point形式，并存储到b_s中，b_s为32行4列\n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "        # 将32个a抽出，转为64-bit integer (signed)形式，并存储到b_a中 (之所以为LongTensor类型，是为了方便后面torch.gather的使用)，b_a为32行1列\n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "        # 将32个r抽出，转为32-bit floating point形式，并存储到b_s中，b_r为32行1列\n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "        # 将32个s_抽出，转为32-bit floating point形式，并存储到b_s中，b_s_为32行4列\n",
    "\n",
    "        # 获取32个transition的评估值和目标值，并利用损失函数和优化器进行评估网络参数更新\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)\n",
    "        # eval_net(b_s)通过评估网络输出32行每个b_s对应的一系列动作值，然后.gather(1, b_a)代表对每行对应索引b_a的Q值提取进行聚合\n",
    "        q_next = self.target_net(b_s_).detach()\n",
    "        # q_next不进行反向传递误差，所以detach；q_next表示通过目标网络输出32行每个b_s_对应的一系列动作值\n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)\n",
    "        # q_next.max(1)[0]表示只返回每一行的最大值，不返回索引(长度为32的一维张量)；.view()表示把前面所得到的一维张量变成(BATCH_SIZE, 1)的形状；最终通过公式得到目标值\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "        # 输入32个评估值和32个目标值，使用均方损失函数\n",
    "        self.optimizer.zero_grad()                                      # 清空上一步的残余更新参数值\n",
    "        loss.backward()                                                 # 误差反向传播, 计算参数更新值\n",
    "        self.optimizer.step()                                           # 更新评估网络的所有参数\n",
    "\n",
    "\n",
    "dqn = DQN()                                                             # 令dqn=DQN类\n",
    "\n",
    "for i in range(400):                                                    # 400个episode循环\n",
    "    print('<<<<<<<<<Episode: %s' % i)\n",
    "    s = env.reset()                                                     # 重置环境\n",
    "    episode_reward_sum = 0                                              # 初始化该循环对应的episode的总奖励\n",
    "\n",
    "    while True:                                                         # 开始一个episode (每一个循环代表一步)\n",
    "        env.render()                                                    # 显示实验动画\n",
    "        a = dqn.choose_action(s)                                        # 输入该步对应的状态s，选择动作\n",
    "        s_, r, done, info = env.step(a)                                 # 执行动作，获得反馈\n",
    "\n",
    "        # 修改奖励 (不修改也可以，修改奖励只是为了更快地得到训练好的摆杆)\n",
    "        x, x_dot, theta, theta_dot = s_\n",
    "        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
    "        new_r = r1 + r2\n",
    "\n",
    "        dqn.store_transition(s, a, new_r, s_)                 # 存储样本\n",
    "        episode_reward_sum += new_r                           # 逐步加上一个episode内每个step的reward\n",
    "\n",
    "        s = s_                                                # 更新状态\n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:              # 如果累计的transition数量超过了记忆库的固定容量2000\n",
    "            # 开始学习 (抽取记忆，即32个transition，并对评估网络参数进行更新，并在开始学习后每隔100次将评估网络的参数赋给目标网络)\n",
    "            dqn.learn()\n",
    "\n",
    "        if done:       # 如果done为True\n",
    "            # round()方法返回episode_reward_sum的小数点四舍五入到2个数字\n",
    "            print('episode%s---reward_sum: %s' % (i, round(episode_reward_sum, 2)))\n",
    "            break                                             # 该episode结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import hashlib\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict\n",
    "\n",
    "class TreeNode:\n",
    "    \"\"\"带哈希缓存的树节点\"\"\"\n",
    "    def __init__(self, id: int):\n",
    "        self.id = id\n",
    "        self.left: TreeNode = None\n",
    "        self.right: TreeNode = None\n",
    "        self.parent: TreeNode = None\n",
    "        self._hash = None\n",
    "        \n",
    "    def subtree_hash(self) -> str:\n",
    "        \"\"\"生成唯一子树标识\"\"\"\n",
    "        if self._hash is None:\n",
    "            serial = self.serialize()\n",
    "            self._hash = hashlib.md5(serial.encode()).hexdigest()\n",
    "        return self._hash\n",
    "    \n",
    "    def serialize(self) -> str:\n",
    "        \"\"\"序列化子树结构\"\"\"\n",
    "        if self.is_leaf:\n",
    "            return str(self.id)\n",
    "        return f\"({self.id}[{self.left.serialize()},{self.right.serialize()}])\"\n",
    "    \n",
    "    @property\n",
    "    def is_leaf(self) -> bool:\n",
    "        return self.left is None and self.right is None\n",
    "\n",
    "class TreeBuilderEnv:\n",
    "    \"\"\"树构建环境\"\"\"\n",
    "    def __init__(self, M: np.ndarray, nodes: List[int], max_depth=10):\n",
    "        self.M = M          # 连接矩阵\n",
    "        self.nodes = nodes # 可用节点\n",
    "        self.max_depth = max_depth\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self) -> Dict:\n",
    "        \"\"\"初始化环境状态\"\"\"\n",
    "        self.root = TreeNode(np.random.choice(self.nodes))\n",
    "        self.used = {self.root.id}\n",
    "        self.available = [n for n in self.nodes if n not in self.used]\n",
    "        self.join_nodes = {}  # 记录合并节点\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self) -> Dict:\n",
    "        \"\"\"生成多维状态表示\"\"\"\n",
    "        state = {\n",
    "            'current_tree': self.root,\n",
    "            'available_nodes': self.available.copy(),\n",
    "            'join_nodes': list(self.join_nodes.values())\n",
    "        }\n",
    "        return state\n",
    "    \n",
    "    def get_legal_actions(self) -> List[tuple]:\n",
    "        \"\"\"生成合法动作列表：(父节点ID, 子节点ID)\"\"\"\n",
    "        actions = []\n",
    "        # 现有节点（包括合并节点）\n",
    "        candidates = list(self.used) + list(self.join_nodes.keys())\n",
    "        for parent in candidates:\n",
    "            for child in self.available:\n",
    "                if self._can_connect(parent, child):\n",
    "                    actions.append((parent, child))\n",
    "        return actions\n",
    "    \n",
    "    def _can_connect(self, parent: int, child: int) -> bool:\n",
    "        \"\"\"检查连接合法性\"\"\"\n",
    "        # 普通节点连接规则\n",
    "        if parent in self.nodes:\n",
    "            return self.M[parent][child] == 1\n",
    "        # 合并节点连接规则（可自定义）\n",
    "        return True\n",
    "    \n",
    "    def step(self, action: tuple) -> tuple:\n",
    "        \"\"\"执行连接动作\"\"\"\n",
    "        parent_id, child_id = action\n",
    "        parent = self._get_node(parent_id)\n",
    "        child = TreeNode(child_id)\n",
    "        \n",
    "        # 连接节点\n",
    "        if parent.left is None:\n",
    "            parent.left = child\n",
    "        elif parent.right is None:\n",
    "            parent.right = child\n",
    "        else:\n",
    "            raise ValueError(\"父节点已满\")\n",
    "        \n",
    "        # 生成合并节点\n",
    "        if parent.left and parent.right:\n",
    "            join_id = f\"join_{parent_id}_{child_id}\"\n",
    "            self.join_nodes[join_id] = parent\n",
    "        \n",
    "        # 更新状态\n",
    "        self.used.add(child_id)\n",
    "        self.available.remove(child_id)\n",
    "        \n",
    "        done = len(self.available) == 0 or len(self.get_legal_actions()) == 0\n",
    "        return self._get_state(), 0, done, {}\n",
    "\n",
    "    def _get_node(self, node_id: int) -> TreeNode:\n",
    "        \"\"\"根据ID获取节点实例\"\"\"\n",
    "        if node_id in self.join_nodes:\n",
    "            return self.join_nodes[node_id]\n",
    "        return self.root if node_id == self.root.id else None\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"带动作掩码的策略网络\"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.action_head = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor, action_mask: torch.Tensor = None):\n",
    "        x = self.encoder(state)\n",
    "        scores = self.action_head(x)\n",
    "        \n",
    "        # 应用动作掩码\n",
    "        if action_mask is not None:\n",
    "            scores = scores - (1 - action_mask.float()) * 1e9\n",
    "        return torch.softmax(scores, dim=-1)\n",
    "\n",
    "class SubtreeOptimizer:\n",
    "    \"\"\"批量子树优化器\"\"\"\n",
    "    def __init__(self, M: np.ndarray, nodes: List[int], batch_size=8, gamma=0.95):\n",
    "        self.M = M\n",
    "        self.nodes = nodes\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # 初始化策略网络\n",
    "        state_dim = len(nodes) * 3  # 节点状态、可用性、深度\n",
    "        self.policy = PolicyNetwork(state_dim)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=3e-4)\n",
    "        \n",
    "    def _get_subtree_reward(self, batch_trees: List[TreeNode]) -> float:\n",
    "        \"\"\"计算公共子树奖励\"\"\"\n",
    "        counter = defaultdict(int)\n",
    "        for tree in batch_trees:\n",
    "            self._count_subtrees(tree.root, counter)\n",
    "        return sum((cnt-1)**2 for cnt in counter.values() if cnt > 1) / self.batch_size\n",
    "    \n",
    "    def _count_subtrees(self, node: TreeNode, counter: Dict[str, int]):\n",
    "        \"\"\"递归统计子树\"\"\"\n",
    "        if node is None or node.is_leaf:\n",
    "            return\n",
    "        counter[node.subtree_hash()] += 1\n",
    "        self._count_subtrees(node.left, counter)\n",
    "        self._count_subtrees(node.right, counter)\n",
    "    \n",
    "    def _encode_state(self, state: Dict) -> torch.Tensor:\n",
    "        \"\"\"状态编码为张量\"\"\"\n",
    "        node_status = [1 if n in state['available_nodes'] else 0 for n in self.nodes]\n",
    "        available_conn = [sum(self.M[n][c] for c in self.nodes) for n in self.nodes]\n",
    "        depth_info = [self._get_depth(n, state['current_tree']) for n in self.nodes]\n",
    "        return torch.FloatTensor(node_status + available_conn + depth_info)\n",
    "    \n",
    "    def _get_depth(self, node_id: int, root: TreeNode) -> int:\n",
    "        \"\"\"获取节点深度\"\"\"\n",
    "        def _depth(node: TreeNode, current: int) -> int:\n",
    "            if node is None: return 0\n",
    "            if node.id == node_id: return current\n",
    "            return max(_depth(node.left, current+1), _depth(node.right, current+1))\n",
    "        return _depth(root, 0)\n",
    "    \n",
    "    def train_batch(self) -> float:\n",
    "        \"\"\"执行批量训练\"\"\"\n",
    "        batch_trees = []\n",
    "        trajectories = []\n",
    "        \n",
    "        # 生成轨迹\n",
    "        for _ in range(self.batch_size):\n",
    "            env = TreeBuilderEnv(self.M, self.nodes)\n",
    "            state = env.reset()\n",
    "            trajectory = []\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                legal_actions = env.get_legal_actions()\n",
    "                if not legal_actions: break\n",
    "                \n",
    "                # 生成动作掩码\n",
    "                action_mask = torch.zeros(len(self.nodes)**2)\n",
    "                for (p, c) in legal_actions:\n",
    "                    idx = self.nodes.index(p)*len(self.nodes) + self.nodes.index(c)\n",
    "                    action_mask[idx] = 1\n",
    "                \n",
    "                # 选择动作\n",
    "                state_tensor = self._encode_state(state)\n",
    "                action_probs = self.policy(state_tensor, action_mask)\n",
    "                action_idx = torch.multinomial(action_probs, 1).item()\n",
    "                parent = self.nodes[action_idx // len(self.nodes)]\n",
    "                child = self.nodes[action_idx % len(self.nodes)]\n",
    "                \n",
    "                # 执行动作\n",
    "                next_state, reward, done, _ = env.step((parent, child))\n",
    "                trajectory.append((state_tensor, action_idx, 0))  # 即时奖励为0\n",
    "                state = next_state\n",
    "            \n",
    "            batch_trees.append(env.root)\n",
    "            trajectories.append(trajectory)\n",
    "        \n",
    "        # 计算奖励\n",
    "        total_reward = self._get_subtree_reward(batch_trees)\n",
    "        \n",
    "        # 策略梯度更新\n",
    "        policy_loss = []\n",
    "        for traj in trajectories:\n",
    "            returns = 0\n",
    "            for t in reversed(range(len(traj))):\n",
    "                state_tensor, action_idx, _ = traj[t]\n",
    "                returns = self.gamma * returns + total_reward\n",
    "                log_prob = torch.log(self.policy(state_tensor)[action_idx])\n",
    "                policy_loss.append(-log_prob * returns)\n",
    "        \n",
    "        # 反向传播\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = torch.stack(policy_loss).mean()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 参数配置\n",
    "    n = 6  # 节点数量\n",
    "    M = np.random.randint(0, 2, (n, n))  # 随机连接矩阵\n",
    "    np.fill_diagonal(M, 0)  # 禁止自连接\n",
    "    nodes = list(range(n))\n",
    "    \n",
    "    # 初始化优化器\n",
    "    optimizer = SubtreeOptimizer(M, nodes, batch_size=8)\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(1000):\n",
    "        loss = optimizer.train_batch()\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    # 生成最终树结构\n",
    "    final_trees = [optimizer.train_batch() for _ in range(5)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
