{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test--节点插入、删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value2\n",
      "{'key': 'value'}\n",
      "{'key': 'value2'}\n",
      "{}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#添加节点\n",
    " \n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "G1 = nx.DiGraph()     \n",
    "# G1.add_node(1,key = \"value\")            #建立一个空的无向图G\n",
    "G1.add_nodes_from([(1,{\"key\":\"value\"}),(2, {\"key\":\"value2\"}),3])                  #添加一个节点1\n",
    "G1.add_edges_from([(1,2)])\n",
    "G1.nodes[1]['key']\n",
    "for node_index in G1.successors(1):\n",
    "    print(G1.nodes[node_index]['key'])\n",
    "\n",
    "G2 = nx.DiGraph()\n",
    "G2.add_nodes_from([1,2])\n",
    "G = nx.disjoint_union(G1, G2)\n",
    "\n",
    "for node, node_data in G1.nodes.items():\n",
    "    print(node_data)\n",
    "\n",
    "nx.draw(G, with_labels=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test--后继节点、子图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct successors of 'A': ['D', 'E', 'C', 'B']\n",
      "Subgraph nodes: ['A', 'B', 'D']\n",
      "Subgraph edges: [('A', 'B'), ('B', 'D')]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# 创建有向图\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# 添加边\n",
    "edges = [\n",
    "    ('A', 'B'),\n",
    "    ('A', 'C'),\n",
    "    ('B', 'D'),\n",
    "    ('C', 'D'),\n",
    "    ('C', 'E'),\n",
    "    ('D', 'E')\n",
    "]\n",
    "\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# nx.draw(G, with_labels=True)\n",
    "\n",
    "# 指定起始节点\n",
    "start_node = 'A'\n",
    "\n",
    "# 获取所有直接后继节\n",
    "successors = list(nx.descendants(G, 'A'))\n",
    "print(\"Direct successors of '{}': {}\".format(start_node, successors))\n",
    "\n",
    "# 创建子图的节点列表（包括起始节点和其后继）\n",
    "subgraph_nodes = [start_node] + successors\n",
    "\n",
    "# 生成子图\n",
    "H = G.subgraph(subgraph_nodes)\n",
    "\n",
    "G.remove_nodes_from([\"E\", \"C\"])\n",
    "\n",
    "\n",
    "# 打印子图的节点和边\n",
    "print(\"Subgraph nodes:\", G.nodes())\n",
    "print(\"Subgraph edges:\", G.edges())\n",
    "\n",
    "# 可视化子图（可选）\n",
    "nx.draw(G, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test--nextworkx转化为pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Data(x=[3, 1], edge_index=[2, 2])\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def convert_nx_to_pyg(G):\n",
    "    # 获取所有节点并排序，确保顺序一致\n",
    "    nodes = list(G.nodes())\n",
    "    node_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "\n",
    "    # 生成节点特征（例如，每个节点一个特征值为1）\n",
    "    n_nodes = len(nodes)\n",
    "    x = torch.ones(n_nodes, 1, dtype=torch.float32)\n",
    "    print(x)\n",
    "    # 处理边信息\n",
    "    edges = list(G.edges())\n",
    "    sources = [node_idx[e[0]] for e in edges]\n",
    "    targets = [node_idx[e[1]] for e in edges]\n",
    "\n",
    "    edge_index = torch.tensor([sources, targets], dtype=torch.long)\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "# 创建示例图\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from([(0, 1), (1, 2)])\n",
    "\n",
    "# 转换图\n",
    "data = convert_nx_to_pyg(G)\n",
    "\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test--自定义环境(继承gym.Env) + DQN + RlLib(强化学习框架)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 09:59:49,469\tINFO worker.py:1841 -- Started a local Ray instance.\n",
      "2025-02-14 09:59:50,425\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-02-14 10:17:25</td></tr>\n",
       "<tr><td>Running for: </td><td>00:17:34.95        </td></tr>\n",
       "<tr><td>Memory:      </td><td>118.0/251.3 GiB    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 4.0/48 CPUs, 0/2 GPUs (0.0/1.0 accelerator_type:A30)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_CustomEnv-v0_5fff5_00000</td><td>RUNNING </td><td>49.52.27.20:34900</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 09:59:50,443\tWARNING algorithm_config.py:4726 -- You are running DQN on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2025-02-14 09:59:50,444\tWARNING algorithm_config.py:4726 -- You are running DQN on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(pid=34900)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "\u001b[36m(DQN pid=34900)\u001b[0m 2025-02-14 09:59:54,019\tWARNING algorithm_config.py:4726 -- You are running DQN on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35038)\u001b[0m 2025-02-14 09:59:57,777\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(DQN pid=34900)\u001b[0m 2025-02-14 09:59:58,257\tWARNING algorithm_config.py:4726 -- You are running DQN on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(DQN pid=34900)\u001b[0m 2025-02-14 09:59:58,273\tWARNING rl_module.py:419 -- Could not create a Catalog object for your RLModule! If you are not using the new API stack yet, make sure to switch it off in your config: `config.api_stack(enable_rl_module_and_learner=False, enable_env_runner_and_connector_v2=False)`. All algos use the new stack by default. Ignore this message, if your RLModule does not use a Catalog to build its sub-components.\n",
      "\u001b[36m(DQN pid=34900)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[36m(pid=35037)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   logger.deprecation(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   logger.deprecation(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m 2025-02-14 09:59:59,166\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     samples = self._sample(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m               ^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     observations, infos = self._try_env_reset()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 159, in _try_env_reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     raise e\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     obs, infos = self.env.reset()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m                  ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/vector/dict_info_to_list.py\", line 95, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/vector/sync_vector_env.py\", line 183, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 400, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     return super().reset(seed=seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/core.py\", line 328, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     return self.env.reset(seed=seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m     result = env.reset(**kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35035)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5a53efdf420527e542a1f58e01000000 Worker ID: a7277ab932301c8614c5917b2e11662ffab04fb5d2a2b514048ec028 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 26118 Worker PID: 35037 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:00:59,169\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN pid=34900)\u001b[0m 2025-02-14 09:59:58,242\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   logger.deprecation(\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m 2025-02-14 09:59:59,166\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     samples = self._sample(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     raise e\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=35037)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:00,452\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:00,452\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:00,453\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:00,453\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:00,453\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:00,453\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:00,453\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=36440)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:02,453\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:04,453\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:04,456\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:04,457\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:01:04,457\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m 2025-02-14 10:01:02,788\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   logger.deprecation(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   logger.deprecation(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m 2025-02-14 10:01:04,469\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     return func(self, *args, **kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     samples = self._sample(\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m               ^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     observations, infos = self._try_env_reset()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 159, in _try_env_reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     raise e\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     obs, infos = self.env.reset()\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m                  ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/vector/dict_info_to_list.py\", line 95, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/vector/sync_vector_env.py\", line 183, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 400, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     return super().reset(seed=seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/core.py\", line 328, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     return self.env.reset(seed=seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m     result = env.reset(**kwargs)\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36440)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:04,472\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=36438)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   logger.deprecation(\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m 2025-02-14 10:01:04,469\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     samples = self._sample(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     raise e\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=36438)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5a53efdf420527e542a1f58e01000000 Worker ID: 5ef67aa513d4d4f93ba836981ad094cbdf14dd72d9b9313af2228f34 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 10729 Worker PID: 36439 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:06,461\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:06,461\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:06,461\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:06,461\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:06,462\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:06,462\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:06,462\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37661)\u001b[0m 2025-02-14 10:02:07,937\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:08,462\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:08,465\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:08,465\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:02:08,466\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:08,480\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=37663)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m 2025-02-14 10:02:08,477\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=37662)\u001b[0m 2025-02-14 10:02:08,022\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5a53efdf420527e542a1f58e01000000 Worker ID: d7a22fde68cfd3ed649c02563cf64124d81bc6f551c796ee806ea41d Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 31367 Worker PID: 37662 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:10,470\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:10,470\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:10,470\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:10,470\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:10,470\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:10,470\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:10,470\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:12,470\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:12,473\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:12,474\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:03:12,474\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffe714c762e5b6504c5ec14c0d01000000 Worker ID: a5afbe94be16f3ce67210d6447a65000ed8a65174b3b56f723cfd0f6 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 33585 Worker PID: 38923 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:12,488\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=38922)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m 2025-02-14 10:03:12,485\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=38922)\u001b[0m 2025-02-14 10:03:12,114\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:14,477\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:14,478\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:14,478\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:14,478\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:14,478\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:14,478\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:14,478\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:16,478\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:16,481\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:16,481\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:04:16,481\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffe714c762e5b6504c5ec14c0d01000000 Worker ID: 7d521623678ef5a150dfab1c3eb58e4d418350ce839d92a30df5ae49 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 28264 Worker PID: 40150 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:16,496\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=40149)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m 2025-02-14 10:04:16,493\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40149)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=40148)\u001b[0m 2025-02-14 10:04:16,081\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:18,486\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:18,486\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:18,486\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:18,486\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:18,486\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:18,486\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:18,486\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:20,487\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:20,490\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:20,490\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:05:20,490\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:20,505\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=41244)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m 2025-02-14 10:05:20,502\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=41244)\u001b[0m 2025-02-14 10:05:20,211\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffe714c762e5b6504c5ec14c0d01000000 Worker ID: 47354d8261ac8bcaf0cf8b7a0088ecd966cb92abdedfd63dc3bfb535 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 36708 Worker PID: 41242 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:22,495\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:22,496\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: RpcError: RPC Error message: Cancelling all calls; RPC Error details: . The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:22,496\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:22,496\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:22,496\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: RpcError: RPC Error message: Cancelling all calls; RPC Error details: . The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:22,496\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:22,496\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:24,496\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:24,499\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:24,499\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:06:24,499\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff7c669962a10c460863c7229c01000000 Worker ID: 8443272f76022987cf227401bcdc0d538f0476f7d4a0dc02f4db1eb0 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 11942 Worker PID: 42396 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:24,514\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=42396)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m 2025-02-14 10:06:24,511\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=42396)\u001b[0m 2025-02-14 10:06:24,171\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:26,505\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:26,505\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:26,505\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:26,505\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:26,505\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:26,505\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:26,505\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:28,505\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:28,509\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:28,509\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:07:28,509\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffe714c762e5b6504c5ec14c0d01000000 Worker ID: 249f851cc279d3e943389c3728560611ced62b5addba96b4b2397e2b Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 22493 Worker PID: 43606 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:28,523\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=43605)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m 2025-02-14 10:07:28,520\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43604)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=43605)\u001b[0m 2025-02-14 10:07:28,128\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:30,514\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:30,514\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:30,514\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:30,515\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:30,515\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:30,515\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:30,515\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:32,515\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:32,518\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:32,518\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:08:32,518\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffe714c762e5b6504c5ec14c0d01000000 Worker ID: 5218125dc9b298d5c2778043e0dd808f94b139f62a7b13506f527b03 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 29905 Worker PID: 44767 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:32,532\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=44767)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m 2025-02-14 10:08:32,529\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=44767)\u001b[0m 2025-02-14 10:08:32,116\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:34,522\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:34,523\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:34,523\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:34,523\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:34,523\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:34,523\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:34,523\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:36,523\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:36,526\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:36,526\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:09:36,526\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5a53efdf420527e542a1f58e01000000 Worker ID: 4669a199656d83527259bb10707a2138ba99af593ba528754e3f84ba Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 30943 Worker PID: 45887 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:36,540\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=45887)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m 2025-02-14 10:09:36,538\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=45887)\u001b[0m 2025-02-14 10:09:36,182\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:38,530\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: RpcError: RPC Error message: Cancelling all calls; RPC Error details: . The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:38,530\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:38,531\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:38,531\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: RpcError: RPC Error message: Cancelling all calls; RPC Error details: . The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:38,531\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:38,531\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:38,531\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:40,531\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:40,534\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:40,534\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:10:40,534\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5a53efdf420527e542a1f58e01000000 Worker ID: b7118d7f1aae3fbf366677981d6e234e3420f8675b00cce6eb3d2b72 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 14141 Worker PID: 47097 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:40,548\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=47097)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m 2025-02-14 10:10:40,546\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=47097)\u001b[0m 2025-02-14 10:10:40,100\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:42,540\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:42,540\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:42,540\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:42,540\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:42,540\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:42,540\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:42,540\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:44,540\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:44,544\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:44,544\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:11:44,544\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffe714c762e5b6504c5ec14c0d01000000 Worker ID: ac65c1da51714d51090f3f7ce1d4f6ca251ddb9bf4edc4c2de2ce531 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 10445 Worker PID: 48295 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:44,559\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=48293)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m 2025-02-14 10:11:44,556\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=48293)\u001b[0m 2025-02-14 10:11:44,121\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:46,549\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:46,549\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:46,550\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:46,550\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:46,550\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:46,550\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:46,550\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:48,549\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:48,552\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:48,552\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:12:48,553\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5a53efdf420527e542a1f58e01000000 Worker ID: 4f3982e098a1fb96efa285f248d2b147ca185b3c6ce46caa73351e5d Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 22975 Worker PID: 49482 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:48,568\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=49482)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m 2025-02-14 10:12:48,565\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=49482)\u001b[0m 2025-02-14 10:12:48,166\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:50,558\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:50,558\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:50,558\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:50,558\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:50,559\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:50,559\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:50,559\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:52,559\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:52,562\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:52,562\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:13:52,562\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:52,577\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=50641)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m 2025-02-14 10:13:52,574\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=50641)\u001b[0m 2025-02-14 10:13:52,257\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff7c669962a10c460863c7229c01000000 Worker ID: e6ee381bebd517f2dd06813eabfeae86f2d3e05de145268f2ac305b2 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 19454 Worker PID: 50639 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:54,568\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:54,568\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:54,568\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:54,569\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:54,569\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:54,569\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:54,569\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:56,569\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:56,572\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:56,572\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:14:56,572\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5a53efdf420527e542a1f58e01000000 Worker ID: 94d79a6be5cf806aa07e8ae23a53eaf9623463dd0b623bcb6b06c1d9 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 12146 Worker PID: 52032 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:15:56,586\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=52030)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m 2025-02-14 10:14:56,583\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=52030)\u001b[0m 2025-02-14 10:14:56,167\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:15:58,578\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:15:58,578\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:15:58,578\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:15:58,578\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:15:58,579\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:15:58,579\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:15:58,579\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:16:00,579\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:16:00,582\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:16:00,582\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:16:00,582\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:00,597\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(pid=53278)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m 2025-02-14 10:16:00,594\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53278)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=53276)\u001b[0m 2025-02-14 10:16:00,177\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff5a53efdf420527e542a1f58e01000000 Worker ID: 17e7458f38eb0ea1075e38a3bcbe7e1eaae40831e229ecfdb446fc58 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 27299 Worker PID: 53278 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:02,588\tERROR actor_manager.py:815 -- Ray error (The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 1 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:02,588\tERROR actor_manager.py:815 -- Ray error (The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: RpcError: RPC Error message: Cancelling all calls; RPC Error details: . The task may or maynot have been executed on the actor.), taking actor 2 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:02,588\tERROR actor_manager.py:815 -- Ray error (The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.), taking actor 3 out of service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:02,588\tERROR actor_manager.py:646 -- The actor e714c762e5b6504c5ec14c0d01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:02,589\tERROR actor_manager.py:646 -- The actor 7c669962a10c460863c7229c01000000 is unavailable: The actor is temporarily unavailable: RpcError: RPC Error message: Cancelling all calls; RPC Error details: . The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:02,589\tERROR actor_manager.py:646 -- The actor 5a53efdf420527e542a1f58e01000000 is unavailable: The actor is temporarily unavailable: IOError: Fail all inflight tasks due to actor state change.. The task may or maynot have been executed on the actor.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m NoneType: None\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:02,589\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:04,589\tWARNING rollout_ops.py:121 -- No samples returned from remote workers. If you have a slow environment or model, consider increasing the `sample_timeout_s` or decreasing the `rollout_fragment_length` in `AlgorithmConfig.env_runners().\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:04,592\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 1 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:04,592\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 2 back into service.\n",
      "\u001b[36m(DQN(env=CustomEnv-v0; env-runners=3; learners=0; multi-agent=False) pid=34900)\u001b[0m 2025-02-14 10:17:04,592\tWARNING actor_manager.py:828 -- Bringing previously unhealthy, now-healthy actor 3 back into service.\n",
      "2025-02-14 10:17:25,385\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2025-02-14 10:17:25,388\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/data/homedata/lch/ray_results/DQN_2025-02-14_09-59-50' in 0.0019s.\n",
      "2025-02-14 10:17:35,395\tINFO tune.py:1041 -- Total run time: 1064.97 seconds (1054.95 seconds for the tuning loop).\n",
      "2025-02-14 10:17:35,396\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n",
      "\u001b[36m(pid=54468)\u001b[0m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:167: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   logger.deprecation(\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m /data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py:180: DeprecationWarning: \u001b[33mWARN: Current gymnasium version requires that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m 2025-02-14 10:17:04,603\tERROR actor_manager.py:187 -- Worker exception caught during `apply()`: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m Traceback (most recent call last):\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/utils/actor_manager.py\", line 183, in apply\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     return func(self, *args, **kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/execution/rollout_ops.py\", line 110, in <lambda>\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     else (lambda w: (w.sample(**random_action_kwargs), w.get_metrics()))\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/util/tracing/tracing_helper.py\", line 463, in _resume_span\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     return method(self, *_args, **_kwargs)\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 205, in sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     samples = self._sample(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m               ^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 260, in _sample\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     self._reset_envs(episodes, shared_data, explore)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/single_agent_env_runner.py\", line 725, in _reset_envs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     observations, infos = self._try_env_reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m                           ^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/ray/rllib/env/env_runner.py\", line 143, in _try_env_reset\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     raise e\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     obs, infos = self.env.reset()\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m                  ^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 293, in reset\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     obs, infos = self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     env_obs, env_info = env.reset(seed=single_seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     return super().reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     return self.env.reset(seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     return env_reset_passive_checker(self.env, seed=seed, options=options)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m   File \"/data/homedata/lch/.local/lib/python3.12/site-packages/gymnasium/utils/passive_env_checker.py\", line 185, in env_reset_passive_checker\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m     result = env.reset(**kwargs)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m              ^^^^^^^^^^^^^^^^^^^\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m TypeError: CustomEnv.reset() got an unexpected keyword argument 'seed'\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SingleAgentEnvRunner pid=54468)\u001b[0m 2025-02-14 10:17:04,176\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 3x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff7c669962a10c460863c7229c01000000 Worker ID: 7caa363a18c1897ff15224680c4e41a00982b2fcd0d550e4b8334814 Node ID: 9798f6cdac4df82c223a14e689da0d4e4cb3b36cba83fae80bcebd65 Worker IP address: 49.52.27.20 Worker port: 32405 Worker PID: 53276 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker exits unexpectedly. Worker exits with an exit code 1.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class CustomEnv(gymnasium.Env):\n",
    "    metadata = {'render.modes': ['console']}\n",
    "\n",
    "    def __init__(self, seed=None):\n",
    "        super().__init__()\n",
    "        self.n_actions = 2  # 动作空间的维度\n",
    "        self.n_states = 3   # 状态空间的维度\n",
    "        self.action_space = spaces.Discrete(self.n_actions)\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(self.n_states,), dtype=np.float32)\n",
    "\n",
    "        self._seed = seed\n",
    "        if self._seed is not None:\n",
    "            random.seed(self._seed)\n",
    "            np.random.seed(self._seed)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment and return the initial state\n",
    "        self.state = np.array([np.random.uniform(-1, 1) for _ in range(self.n_states)])\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Execute one step in the environment\n",
    "        reward = random.uniform(0, 1)\n",
    "        done = False  # Episodes are not terminal by default\n",
    "        next_state = np.array([np.random.uniform(-1, 1) for _ in range(self.n_states)])\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def render(self, mode='console'):\n",
    "        print(f\"Current state: {self.state}\")\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def is_continuous(self):\n",
    "        return False\n",
    "\n",
    "    def is_discrete(self):\n",
    "        return True\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "### DQN算法实现部分（使用PyTorch）\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DQN_Agent:\n",
    "    def __init__(self, env, batch_size=32, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=10000):\n",
    "        self.env = env\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        input_size = env.observation_space.shape[0]\n",
    "        output_size = env.action_space.n\n",
    "        self.model = DQN(input_size, output_size)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randint(0, self.env.action_space.n - 1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(torch.FloatTensor(state))\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = np.array([t[0] for t in minibatch])\n",
    "        actions = np.array([t[1] for t in minibatch]).astype(np.int64)\n",
    "        rewards = np.array([t[2] for t in minibatch]).astype(np.float32)\n",
    "        next_states = np.array([t[3] for t in minibatch])\n",
    "        dones = np.array([t[4] for t in minibatch]).astype(bool)\n",
    "\n",
    "        # Convert to tensors\n",
    "        states_t = torch.FloatTensor(states)\n",
    "        actions_t = actions.long()\n",
    "        rewards_t = rewards.unsqueeze(1)  # 增加一个维度，方便后续计算\n",
    "        next_states_t = torch.FloatTensor(next_states)\n",
    "        dones_t = dones.float().unsqueeze(1)\n",
    "\n",
    "        current_q_values = self.model(states_t).gather(dim=1, index=actions_t.unsqueeze(1))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.model(next_states_t).max(dim=1)[0].detach()\n",
    "\n",
    "        target_q_values = rewards_t + (1 - dones_t) * self.gamma * next_q_values.unsqueeze(1)\n",
    "\n",
    "        loss = self.criterion(current_q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon_decay / (self.epsilon_decay + np.exp(-self.epsilon)))\n",
    "\n",
    "### 使用RLlib整合代码\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.env import BaseEnv\n",
    "\n",
    "# 将自定义环境注册到Ray/RLLib\n",
    "def env_creator(env_config):\n",
    "    return CustomEnv()\n",
    "\n",
    "register_env(\"CustomEnv-v0\", env_creator)\n",
    "\n",
    "# DQN模型的配置\n",
    "config = {\n",
    "    \"env\": \"CustomEnv-v0\",\n",
    "    \"num_workers\": 3,\n",
    "    \"gamma\": 0.99,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"buffer_size\": 10000,\n",
    "    \"batch_size\": 32,\n",
    "    \"epsilon_start\": 1.0,\n",
    "    \"epsilon_end\": 0.01,\n",
    "    \"epsilon_decay\": 10000,\n",
    "}\n",
    "\n",
    "ray.init()\n",
    "\n",
    "# 启动训练过程\n",
    "tune.run(\n",
    "    \"DQN\",\n",
    "    config=config,\n",
    "    stop={\"episode_reward_mean\": 500},\n",
    ")\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test--numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "v1 = np.zeros((0,2))\n",
    "v2 = np.array([3,4]).reshape(1,-1)\n",
    "v3 = np.array([5, 6]).reshape(1,-1)\n",
    "v = np.vstack((v1, v2,v3))\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test--Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()\n",
    "v1 = torch.Tensor([1,2])\n",
    "v2 = torch.Tensor([3,4])\n",
    "v = torch.concatenate((v1,v2))\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test--强化学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: 10, Epsilon: 0.90\n",
      "Episode: 1, Total Reward: 10, Epsilon: 0.83\n",
      "Episode: 2, Total Reward: 10, Epsilon: 0.78\n",
      "Episode: 3, Total Reward: 10, Epsilon: 0.70\n",
      "Episode: 4, Total Reward: 10, Epsilon: 0.61\n",
      "Episode: 5, Total Reward: 10, Epsilon: 0.58\n",
      "Episode: 6, Total Reward: 10, Epsilon: 0.47\n",
      "Episode: 7, Total Reward: 10, Epsilon: 0.45\n",
      "Episode: 8, Total Reward: 10, Epsilon: 0.42\n",
      "Episode: 9, Total Reward: 10, Epsilon: 0.40\n",
      "Episode: 10, Total Reward: 10, Epsilon: 0.38\n",
      "Episode: 11, Total Reward: 10, Epsilon: 0.37\n",
      "Episode: 12, Total Reward: 10, Epsilon: 0.36\n",
      "Episode: 13, Total Reward: 10, Epsilon: 0.34\n",
      "Episode: 14, Total Reward: 10, Epsilon: 0.33\n",
      "Episode: 15, Total Reward: 10, Epsilon: 0.31\n",
      "Episode: 16, Total Reward: 10, Epsilon: 0.30\n",
      "Episode: 17, Total Reward: 10, Epsilon: 0.29\n",
      "Episode: 18, Total Reward: 10, Epsilon: 0.28\n",
      "Episode: 19, Total Reward: 10, Epsilon: 0.25\n",
      "\n",
      "Test Trajectory:\n",
      "Position: [0 0] -> Action: Right -> New Position: [0 1]\n",
      "Position: [0 1] -> Action: Right -> New Position: [0 2]\n",
      "Position: [0 2] -> Action: Down -> New Position: [1 2]\n",
      "Position: [1 2] -> Action: Right -> New Position: [1 3]\n",
      "Position: [1 3] -> Action: Down -> New Position: [2 3]\n",
      "Position: [2 3] -> Action: Down -> New Position: [3 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# 在代码开头添加以下导入语句\n",
    "import torch.nn.functional as F  # PyTorch场景\n",
    "\n",
    "# 环境定义（4x4网格世界）\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.size = 4\n",
    "        self.goal = (3, 3)\n",
    "        self.obstacles = [(1, 1), (2, 2)]\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.pos = (0, 0)\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        return np.array(self.pos)\n",
    "    \n",
    "    def get_mask(self):\n",
    "        x, y = self.pos\n",
    "        mask = [1, 1, 1, 1]  # 上下左右\n",
    "        \n",
    "        # 边界检测\n",
    "        if x == 0: mask[0] = 0       # 不能向上\n",
    "        if x == self.size-1: mask[1] = 0  # 不能向下\n",
    "        if y == 0: mask[3] = 0       # 不能向左\n",
    "        if y == self.size-1: mask[2] = 0  # 不能向右\n",
    "        \n",
    "        # 障碍物检测\n",
    "        for (ox, oy) in self.obstacles:\n",
    "            if (x-1, y) == (ox, oy): mask[0] = 0  # 上方障碍\n",
    "            if (x+1, y) == (ox, oy): mask[1] = 0  # 下方障碍\n",
    "            if (x, y+1) == (ox, oy): mask[2] = 0  # 右方障碍\n",
    "            if (x, y-1) == (ox, oy): mask[3] = 0  # 左方障碍\n",
    "            \n",
    "        return np.array(mask)\n",
    "    \n",
    "    def step(self, action):\n",
    "        x, y = self.pos\n",
    "        reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # 执行动作（带有效性验证）\n",
    "        if self.get_mask()[action]:\n",
    "            if action == 0: new_pos = (x-1, y)\n",
    "            elif action == 1: new_pos = (x+1, y)\n",
    "            elif action == 2: new_pos = (x, y+1)\n",
    "            else: new_pos = (x, y-1)\n",
    "            \n",
    "            # 碰撞检测\n",
    "            if new_pos in self.obstacles:\n",
    "                reward = -1  # 碰撞惩罚\n",
    "            else:\n",
    "                self.pos = new_pos\n",
    "                \n",
    "            # 目标检测（稀疏奖励）\n",
    "            if self.pos == self.goal:\n",
    "                reward = 10\n",
    "                done = True\n",
    "                \n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "# DQN网络\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# 带掩码的DQN智能体\n",
    "class MaskedDQNAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state_dim = 2\n",
    "        self.action_dim = 4\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = 32\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        \n",
    "        self.policy_net = DQN(self.state_dim, self.action_dim)\n",
    "        self.target_net = DQN(self.state_dim, self.action_dim)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.001)\n",
    "        \n",
    "    def choose_action(self, state, mask):\n",
    "        state = torch.FloatTensor(state)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            valid_actions = np.where(mask == 1)[0]\n",
    "            return np.random.choice(valid_actions)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net(state)\n",
    "                # 应用掩码\n",
    "                q_values = torch.where(torch.BoolTensor(mask), q_values, torch.tensor(-np.inf))\n",
    "                return torch.argmax(q_values).item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done, next_mask):\n",
    "        self.memory.append((state, action, reward, next_state, done, next_mask))\n",
    "        \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "    def train(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones, next_masks = zip(*batch)\n",
    "        \n",
    "        states = torch.FloatTensor(np.array(states))\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(np.array(next_states))\n",
    "        dones = torch.BoolTensor(dones)\n",
    "        \n",
    "        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # 带掩码的下一状态Q值计算\n",
    "        next_q = torch.zeros(self.batch_size)\n",
    "        valid_next_states = next_states[~dones]\n",
    "        valid_masks = [m for m, d in zip(next_masks, dones) if not d]\n",
    "        \n",
    "        if len(valid_next_states) > 0:\n",
    "            with torch.no_grad():\n",
    "                next_q_values = self.target_net(valid_next_states)\n",
    "                # 应用下一状态掩码\n",
    "                masked_q = [torch.where(torch.BoolTensor(m), q, torch.tensor(-np.inf)) \n",
    "                           for m, q in zip(valid_masks, next_q_values)]\n",
    "                max_next_q = torch.stack([q.max() for q in masked_q])\n",
    "                next_q[~dones] = max_next_q\n",
    "        \n",
    "        target_q = rewards + self.gamma * next_q\n",
    "        \n",
    "        loss = F.mse_loss(current_q.squeeze(), target_q.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Epsilon衰减\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# 训练流程\n",
    "env = GridWorld()\n",
    "agent = MaskedDQNAgent(env)\n",
    "episodes = 20\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        mask = env.get_mask()\n",
    "        action = agent.choose_action(state, mask)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_mask = env.get_mask() if not done else np.zeros(4)\n",
    "        \n",
    "        agent.store_transition(state, action, reward, next_state, done, next_mask)\n",
    "        agent.train()\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "    if ep % 1 == 0:\n",
    "        agent.update_target()\n",
    "        print(f\"Episode: {ep}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
    "\n",
    "# 测试演示\n",
    "test_env = GridWorld()\n",
    "state = test_env.reset()\n",
    "done = False\n",
    "\n",
    "print(\"\\nTest Trajectory:\")\n",
    "while not done:\n",
    "    mask = test_env.get_mask()\n",
    "    action = agent.choose_action(state, mask)\n",
    "    next_state, reward, done = test_env.step(action)\n",
    "    print(f\"Position: {state} -> Action: {['Up','Down','Right','Left'][action]} -> New Position: {next_state}\")\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test--其他"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t', 'title']\n",
      "{('alias', 'col')}\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "12\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m b \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(b)\n\u001b[0;32m---> 21\u001b[0m a,b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(a)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "d = {'key':'value'}\n",
    "list(d.values())[0]\n",
    "col = 't.title'\n",
    "ans = col.split('.')\n",
    "print(ans)\n",
    "ans = set()\n",
    "ans.add(('alias', 'col'))\n",
    "ans.add(('alias', 'col'))\n",
    "print(ans)\n",
    "a = [(1,2),(3,4)]\n",
    "for b,c in a:\n",
    "    print(b)\n",
    "    print(c)\n",
    "a = '123'\n",
    "a = a[:-1]\n",
    "print(a)\n",
    "a = [1,2,3]\n",
    "b = []\n",
    "\n",
    "print(b)\n",
    "a,b = '1','2'\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
